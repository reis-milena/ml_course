{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and Gain\n",
    "\n",
    "Gain: information gain\n",
    "\n",
    "Important to find out **which attributes are most important** (so it stays at the top of the tree)\n",
    "\n",
    "<img src=\"decision_tree_entropy.png\"  style=\"width: 500px;\"/>\n",
    "\n",
    "S: output\n",
    "A: atribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"decision_tree_entropy_calc.png\"  style=\"width: 700px;\"/>\n",
    "\n",
    "Gain of atribute history: 0.26\n",
    "\n",
    "The smaller the entropy value, the better/bigger the gain of information.\n",
    "\n",
    "<img src=\"decision_tree_entropy_calc2.png\"  style=\"width: 700px;\"/>\n",
    "\n",
    "As income has the biggest gain (0.66), it stays on the top of the tree. To find out about the next atribute it's necessary to calculate the entropy and the gain of the rest of the atributes again, but now filtering by the income.\n",
    "\n",
    "Note that this is a recursive method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see decision tree\n",
    "\n",
    "<img src=\"decision_tree_view2.png\"  style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Poda\" of the decision tree\n",
    "\n",
    "Cut to get rid of bad or unnecessary atributes.\n",
    "\n",
    "Two concepts:\n",
    "\n",
    "- Bias: error by wrong classification\n",
    "\n",
    "- Variance: \n",
    "\n",
    "    - error due to low sensibility to changes in the training data\n",
    "    - overfitting (similar to decorate the training data and not learn with it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros VS Cons\n",
    "\n",
    "Pros:\n",
    "- Easy understanding\n",
    "- It's not necessary to enchelon or normalize the data\n",
    "- Fast\n",
    "\n",
    "Cons:\n",
    "- Complex trees\n",
    "- Small changes on the data can change de tree. Here the \"poda\" can help to deal with it.\n",
    "- ?? NP-complete\n",
    "\n",
    "Upgrades of decision tree: random forest\n",
    "\n",
    "CART: classification and regression trees"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
