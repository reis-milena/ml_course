{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropy and Gain\n",
    "\n",
    "Gain: information gain\n",
    "\n",
    "Important to find out **which attributes are most important** (so it stays at the top of the tree)\n",
    "\n",
    "<img src=\"decision_tree_entropy.png\"  style=\"width: 500px;\"/>\n",
    "\n",
    "S: output\n",
    "A: atribute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "<img src=\"decision_tree_entropy_calc.png\"  style=\"width: 700px;\"/>\n",
    "\n",
    "Gain of atribute history: 0.26\n",
    "\n",
    "The smaller the entropy value, the better/bigger the gain of information.\n",
    "\n",
    "<img src=\"decision_tree_entropy_calc2.png\"  style=\"width: 700px;\"/>\n",
    "\n",
    "As income has the biggest gain (0.66), it stays on the top of the tree. To find out about the next atribute it's necessary to calculate the entropy and the gain of the rest of the atributes again, but now filtering by the income.\n",
    "\n",
    "Note that this is a recursive method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to see decision tree\n",
    "\n",
    "<img src=\"decision_tree_view2.png\"  style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Poda\" of the decision tree\n",
    "\n",
    "Cut to get rid of bad or unnecessary atributes.\n",
    "\n",
    "Two concepts:\n",
    "\n",
    "- Bias: error by wrong classification\n",
    "\n",
    "- Variance: \n",
    "\n",
    "    - error due to low sensibility to changes in the training data\n",
    "    - overfitting (similar to decorate the training data and not learn with it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros VS Cons\n",
    "\n",
    "Pros:\n",
    "- Easy understanding\n",
    "- It's not necessary to enchelon or normalize the data\n",
    "- Fast\n",
    "\n",
    "Cons:\n",
    "- Complex trees\n",
    "- Small changes on the data can change de tree. Here the \"poda\" can help to deal with it.\n",
    "- ?? NP-complete\n",
    "\n",
    "Upgrades of decision tree: random forest\n",
    "\n",
    "CART: classification and regression trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest\n",
    "\n",
    "Improvement to decision tree algorithm.\n",
    "\n",
    "The method uses several decision trees and combines the result of those trees.\n",
    "\n",
    "- Ensemble learning: the idea of consulting several professional to make a decision. In other words, several algorithms together makes one algorithm stronger.\n",
    "    - **Mean** for regression and **bagging** (voting majority) for classification\n",
    "\n",
    "Attention: the more trees used, the greater the chances of overfitting!\n",
    "\n",
    "#### How it works\n",
    "\n",
    "It selects $K$ atributes randomly to compare impurity metrics (gini or entropy) and selects the $N$ number of trees.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"decision_treeVSrandom_forest.png\"  style=\"width: 500px;\"/>"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
