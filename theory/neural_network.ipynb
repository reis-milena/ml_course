{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network\n",
    "\n",
    "Pre-determined algorithm usage: reccommendation system, search - ex: google maps, graph (grafo), ordenation.\n",
    "\n",
    "Recently usage: finding of new medicine, understanding of natural language - translation of a language, autonomous cars, facial recognition, stock exchange, traffic control\n",
    "\n",
    "Usage with **BIG DATA** and **COMPLEX PROBLEMS**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological parallel\n",
    "\n",
    "- Neuron: process information\n",
    "- Axon: send signal from one neuron to another (the connection)\n",
    "\n",
    "Neural network:\n",
    "- Receive an entrance value, the network process it and returns an answer\n",
    "- The neuron is actived if the value is bigger than the limit\n",
    "\n",
    "Logic: entrance -> neuron + axon -> return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial neuron: Single layer\n",
    "\n",
    "Sum = $\\sum^n_{i=1}x_i*w_i$\n",
    "\n",
    "<img src=\"neural_network1.png\"  style=\"width: 300px;\"/>\n",
    "\n",
    "**Step function**: if return value >0 then 1, else 0\n",
    "\n",
    "Too vulnerable to small entrance values\n",
    "\n",
    "Positive weight: more chance to activate the neuron.\n",
    "Negative weight: less chance to activate the neuron.\n",
    "Weight are synapses and it increase/reduce the entrance value\n",
    "\n",
    "The knowledge of the neural networks are the weights. **So the neural network constructs the best weight combination**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting weight\n",
    "\n",
    "- AND logic operator: supervised method. It trains the model to detect if the both of variables are true then the answer is true (=1), else is false(=0)\n",
    "\n",
    "#### Error\n",
    "\n",
    "error = correct answer - predicted answer\n",
    "\n",
    "#### Setting weight\n",
    "\n",
    "weight(n+1) = weight(n) + (learningRate * entrance * error)\n",
    "\n",
    "learningRate: fixed value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayers Perceptron\n",
    "\n",
    "Feed forward: the input value value is processed on the network and generates an answer at the end.\n",
    "\n",
    "<img src=\"neural_network2.png\"  style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "\n",
    "**Binary Step function**: is a simple type of activation function.\n",
    "\n",
    "**Sigmoid function (Logistic aka Soft Step)**: $y = \\frac{1}{1+e^{-x}}$ em que $y \\in (0,1)$\n",
    "\n",
    "    - Higher X, y -> 1\n",
    "    - Lower X, y-> 0\n",
    "\n",
    "**Hyberbolic tangent funciont**: $y = \\frac{e^x-e^{-x}}{e^x+e^{-x}}$ em que $y \\in (-1,1)$\n",
    "\n",
    "There are more functions..\n",
    "\n",
    "Neuron sigmoid uses sigmoid function.\n",
    "\n",
    "Perceptrons uses step function.\n",
    "\n",
    "Obs: sigmoid funciton is widely used and it works well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm\n",
    "\n",
    "1- Starts with random weight\n",
    "\n",
    "2- Calculate the error (using)\n",
    "\n",
    "3- Recalculate weights (backpropagation)\n",
    "\n",
    "4- It ends when the error is small (minimizes error)\n",
    "\n",
    "<img src=\"neural_network3.png\"  style=\"width: 300px;\"/>\n",
    "\n",
    "Cost function = error function\n",
    "\n",
    "Options to update weights and minimize error\n",
    "\n",
    "- Gradient descent\n",
    "- Derivate\n",
    "- Delta rule\n",
    "- Backpropagation\n",
    "- Learning rate\n",
    "- Momentum\n",
    "\n",
    "gradient descent and derivate and delta rule works together:\n",
    "\n",
    "Activation function -> derivate -> delta -> gradient\n",
    "\n",
    "Learning rate and momentum are parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient descent\n",
    "\n",
    "Min $C(w_1,w_2,...w_n)$ by calculating the partial derivate to move into the gradient direction.\n",
    "\n",
    "- Stochatstic gradient descent: for each register calculates error and update weight (and repeats for each register)\n",
    "\n",
    "    - Prevenction for local minimum (non-convex function)\n",
    "\n",
    "    - Faster\n",
    "\n",
    "- Batch gradient descent: calculate error for all registers and then update weight\n",
    "\n",
    "    - Mini batch gradient descent: the same but not with all registers but a smaller subset\n",
    "\n",
    "#### Delta rule\n",
    "\n",
    "**Output Delta** = error * activation function derivate\n",
    "\n",
    "**Hidden Delta** = activation function derivate * weight * Output Delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backpropagation\n",
    "\n",
    "Update error \n",
    "\n",
    "$weight_{n+1}$ = ($weight_n$*momentum) + (input * delta * learningRate)\n",
    "\n",
    "#### Learning rate\n",
    "\n",
    "How fast will the algorithm learn.\n",
    "\n",
    "Higer values: faster convergence, but higher chances to lose global minimum\n",
    "Small values: too slow and may not even achieve global minimum\n",
    "\n",
    "#### Momentum\n",
    "\n",
    "To avoid local minimum.\n",
    "\n",
    "How reliable is the last update.\n",
    "\n",
    "Higher values: faster convergence\n",
    "Small values: avoid local minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "\n",
    "Adds a unit value in each layer, changing the output.\n",
    "\n",
    "### Error (loss)\n",
    "\n",
    "As seen before: error = correct answer - predicted answer\n",
    "\n",
    "Types of error:\n",
    "\n",
    "**Mean Square Error** (MSE): $MSE = \\frac{1}{N}\\sum^N_{i=1}(f_i-y_i)^2$\n",
    "\n",
    "**Root Mean Square Error** (RMSE): $RMSE = \\sqrt{\\frac{1}{N}\\sum^N_{i=1}(f_i-y_i)^2}$\n",
    "\n",
    "- Bigger errors weigh more\n",
    "- More penalties for bigger mistakes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Neurons\n",
    "\n",
    "Usually the number of neurons equals the number of atribute class.\n",
    "\n",
    "Example for risk dataset:\n",
    "\n",
    "<img src=\"neural_network4.png\"  style=\"width: 500px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Learning\n",
    "\n",
    "When there are 2 or more hidden layers.\n",
    "\n",
    "* It's necessary to use different techniques.\n",
    "\n",
    "* Vanishing gradient problem\n",
    "\n",
    "* Other types of activation function\n",
    "\n",
    "How implement:\n",
    "\n",
    "* Convolutional neural network (technique)\n",
    "\n",
    "* Recurrent neural network (technique)\n",
    "\n",
    "* Keras, Theano, TensorFlow (tool)\n",
    "    \n",
    "* GPU (programming with gpu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hidden layers\n",
    "\n",
    "How many neurons on hidden layer: Neurons = $\\frac{input + output}{2}$\n",
    "\n",
    "Linear separation problems does not need hidden layer.\n",
    "\n",
    "- Cross Validation: To find the set of parameters\n",
    "\n",
    "- AutoML\n",
    "\n",
    "- Usually two layers works well with small datasets\n",
    "\n",
    "- More layers are needed on more complex problems\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
