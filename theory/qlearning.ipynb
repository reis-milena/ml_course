{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Different from classification and association because there are no data to learn with it.\n",
    "\n",
    "For reinforcement learning you only need an objective and reward.\n",
    "\n",
    "Environment + agent + action + state and reward\n",
    "\n",
    "- Reward can be positive or negative\n",
    "\n",
    "<img src=\"reinforcement_learning.png\"  style=\"width: 500px;\"/>\n",
    "\n",
    "## Bellman Equation\n",
    "\n",
    "$V(s) = \\max_a(R(s,a)+\\gamma V(s'))$\n",
    "\n",
    "$\\gamma$ discount factor where $({\\displaystyle 0\\leq \\gamma \\leq 1})$.\n",
    "\n",
    "$\\gamma$ it has the effect of valuing rewards received earlier higher than those received later. It may also be interpreted as the probability to succeed (or survive) at every step\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process - MDP\n",
    "\n",
    "**Exploration X Exploitation**\n",
    "\n",
    "- Exploration: will have higher probability to follow the correct path and lower probability to follow the wrong path (not-deterministic). \n",
    "\n",
    "        - Allows to explore the environment\n",
    "\n",
    "- Exploitation: agent will always follow the correct path (deterministic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning\n",
    "\n",
    "Higer $Q(s_i,a_i)$ value, better the solution\n",
    "\n",
    "Calculus of Q(.) value before the V(.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time/temporal difference\n",
    "\n",
    "Episode: complete path from begning to end\n",
    "\n",
    "$Q_t(s,a) = Q_{t-1}(s,a)+\\alpha(R(s,a)+\\gamma V(s'))$\n",
    "\n",
    "$Q_t(s,a) = Q_{t-1}(s,a)+\\alpha TD_t(a,s)$"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
