{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neura Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier #MULTILAYERPERCEPTON CLASSIFIER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('credit.pkl', 'rb') as f:\n",
    "    x_credit_training, y_credit_training, x_credit_test, y_credit_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1500, 3), (1500,), (500, 3), (500,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_credit_training.shape, y_credit_training.shape, x_credit_test.shape, y_credit_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.13658045\n",
      "Iteration 2, loss = 1.12784398\n",
      "Iteration 3, loss = 1.11917941\n",
      "Iteration 4, loss = 1.11068960\n",
      "Iteration 5, loss = 1.10218737\n",
      "Iteration 6, loss = 1.09392107\n",
      "Iteration 7, loss = 1.08533001\n",
      "Iteration 8, loss = 1.07658506\n",
      "Iteration 9, loss = 1.06741291\n",
      "Iteration 10, loss = 1.05788849\n",
      "Iteration 11, loss = 1.04812643\n",
      "Iteration 12, loss = 1.03781095\n",
      "Iteration 13, loss = 1.02724400\n",
      "Iteration 14, loss = 1.01634843\n",
      "Iteration 15, loss = 1.00504572\n",
      "Iteration 16, loss = 0.99322507\n",
      "Iteration 17, loss = 0.98128734\n",
      "Iteration 18, loss = 0.96865547\n",
      "Iteration 19, loss = 0.95586006\n",
      "Iteration 20, loss = 0.94255923\n",
      "Iteration 21, loss = 0.92938113\n",
      "Iteration 22, loss = 0.91543240\n",
      "Iteration 23, loss = 0.90181173\n",
      "Iteration 24, loss = 0.88755681\n",
      "Iteration 25, loss = 0.87355120\n",
      "Iteration 26, loss = 0.85930963\n",
      "Iteration 27, loss = 0.84524582\n",
      "Iteration 28, loss = 0.83103377\n",
      "Iteration 29, loss = 0.81687939\n",
      "Iteration 30, loss = 0.80304115\n",
      "Iteration 31, loss = 0.78909240\n",
      "Iteration 32, loss = 0.77563200\n",
      "Iteration 33, loss = 0.76221899\n",
      "Iteration 34, loss = 0.74917864\n",
      "Iteration 35, loss = 0.73657272\n",
      "Iteration 36, loss = 0.72439332\n",
      "Iteration 37, loss = 0.71247138\n",
      "Iteration 38, loss = 0.70086852\n",
      "Iteration 39, loss = 0.68949607\n",
      "Iteration 40, loss = 0.67878383\n",
      "Iteration 41, loss = 0.66833194\n",
      "Iteration 42, loss = 0.65831674\n",
      "Iteration 43, loss = 0.64880327\n",
      "Iteration 44, loss = 0.63964103\n",
      "Iteration 45, loss = 0.63067280\n",
      "Iteration 46, loss = 0.62227089\n",
      "Iteration 47, loss = 0.61406170\n",
      "Iteration 48, loss = 0.60616539\n",
      "Iteration 49, loss = 0.59862233\n",
      "Iteration 50, loss = 0.59155239\n",
      "Iteration 51, loss = 0.58459262\n",
      "Iteration 52, loss = 0.57784640\n",
      "Iteration 53, loss = 0.57141229\n",
      "Iteration 54, loss = 0.56512477\n",
      "Iteration 55, loss = 0.55907162\n",
      "Iteration 56, loss = 0.55306609\n",
      "Iteration 57, loss = 0.54729794\n",
      "Iteration 58, loss = 0.54175866\n",
      "Iteration 59, loss = 0.53645394\n",
      "Iteration 60, loss = 0.53120065\n",
      "Iteration 61, loss = 0.52604775\n",
      "Iteration 62, loss = 0.52107830\n",
      "Iteration 63, loss = 0.51623943\n",
      "Iteration 64, loss = 0.51160685\n",
      "Iteration 65, loss = 0.50705373\n",
      "Iteration 66, loss = 0.50251621\n",
      "Iteration 67, loss = 0.49817441\n",
      "Iteration 68, loss = 0.49387597\n",
      "Iteration 69, loss = 0.48974329\n",
      "Iteration 70, loss = 0.48560475\n",
      "Iteration 71, loss = 0.48159025\n",
      "Iteration 72, loss = 0.47769529\n",
      "Iteration 73, loss = 0.47372731\n",
      "Iteration 74, loss = 0.47000146\n",
      "Iteration 75, loss = 0.46625599\n",
      "Iteration 76, loss = 0.46240775\n",
      "Iteration 77, loss = 0.45873093\n",
      "Iteration 78, loss = 0.45496803\n",
      "Iteration 79, loss = 0.45115397\n",
      "Iteration 80, loss = 0.44750547\n",
      "Iteration 81, loss = 0.44374701\n",
      "Iteration 82, loss = 0.44008028\n",
      "Iteration 83, loss = 0.43631206\n",
      "Iteration 84, loss = 0.43263410\n",
      "Iteration 85, loss = 0.42884196\n",
      "Iteration 86, loss = 0.42519427\n",
      "Iteration 87, loss = 0.42134047\n",
      "Iteration 88, loss = 0.41759596\n",
      "Iteration 89, loss = 0.41397816\n",
      "Iteration 90, loss = 0.40995432\n",
      "Iteration 91, loss = 0.40558585\n",
      "Iteration 92, loss = 0.40105827\n",
      "Iteration 93, loss = 0.39658776\n",
      "Iteration 94, loss = 0.39216666\n",
      "Iteration 95, loss = 0.38795160\n",
      "Iteration 96, loss = 0.38385510\n",
      "Iteration 97, loss = 0.37967106\n",
      "Iteration 98, loss = 0.37576294\n",
      "Iteration 99, loss = 0.37178611\n",
      "Iteration 100, loss = 0.36798093\n",
      "Iteration 101, loss = 0.36424624\n",
      "Iteration 102, loss = 0.36054173\n",
      "Iteration 103, loss = 0.35704490\n",
      "Iteration 104, loss = 0.35345352\n",
      "Iteration 105, loss = 0.35000659\n",
      "Iteration 106, loss = 0.34672781\n",
      "Iteration 107, loss = 0.34345737\n",
      "Iteration 108, loss = 0.34035764\n",
      "Iteration 109, loss = 0.33739178\n",
      "Iteration 110, loss = 0.33435090\n",
      "Iteration 111, loss = 0.33151578\n",
      "Iteration 112, loss = 0.32867694\n",
      "Iteration 113, loss = 0.32598204\n",
      "Iteration 114, loss = 0.32341775\n",
      "Iteration 115, loss = 0.32099053\n",
      "Iteration 116, loss = 0.31850622\n",
      "Iteration 117, loss = 0.31620658\n",
      "Iteration 118, loss = 0.31396330\n",
      "Iteration 119, loss = 0.31189302\n",
      "Iteration 120, loss = 0.30973911\n",
      "Iteration 121, loss = 0.30777758\n",
      "Iteration 122, loss = 0.30582538\n",
      "Iteration 123, loss = 0.30393301\n",
      "Iteration 124, loss = 0.30207552\n",
      "Iteration 125, loss = 0.30028311\n",
      "Iteration 126, loss = 0.29850798\n",
      "Iteration 127, loss = 0.29665662\n",
      "Iteration 128, loss = 0.29499576\n",
      "Iteration 129, loss = 0.29329296\n",
      "Iteration 130, loss = 0.29161329\n",
      "Iteration 131, loss = 0.28998704\n",
      "Iteration 132, loss = 0.28834753\n",
      "Iteration 133, loss = 0.28675827\n",
      "Iteration 134, loss = 0.28526002\n",
      "Iteration 135, loss = 0.28375082\n",
      "Iteration 136, loss = 0.28229828\n",
      "Iteration 137, loss = 0.28093160\n",
      "Iteration 138, loss = 0.27951602\n",
      "Iteration 139, loss = 0.27812916\n",
      "Iteration 140, loss = 0.27674730\n",
      "Iteration 141, loss = 0.27541706\n",
      "Iteration 142, loss = 0.27411598\n",
      "Iteration 143, loss = 0.27285680\n",
      "Iteration 144, loss = 0.27165570\n",
      "Iteration 145, loss = 0.27044960\n",
      "Iteration 146, loss = 0.26929527\n",
      "Iteration 147, loss = 0.26810852\n",
      "Iteration 148, loss = 0.26695566\n",
      "Iteration 149, loss = 0.26585192\n",
      "Iteration 150, loss = 0.26478503\n",
      "Iteration 151, loss = 0.26370146\n",
      "Iteration 152, loss = 0.26269773\n",
      "Iteration 153, loss = 0.26167318\n",
      "Iteration 154, loss = 0.26070320\n",
      "Iteration 155, loss = 0.25973230\n",
      "Iteration 156, loss = 0.25875012\n",
      "Iteration 157, loss = 0.25782676\n",
      "Iteration 158, loss = 0.25691627\n",
      "Iteration 159, loss = 0.25596166\n",
      "Iteration 160, loss = 0.25502656\n",
      "Iteration 161, loss = 0.25409298\n",
      "Iteration 162, loss = 0.25315769\n",
      "Iteration 163, loss = 0.25225332\n",
      "Iteration 164, loss = 0.25131087\n",
      "Iteration 165, loss = 0.25038424\n",
      "Iteration 166, loss = 0.24942648\n",
      "Iteration 167, loss = 0.24848827\n",
      "Iteration 168, loss = 0.24753503\n",
      "Iteration 169, loss = 0.24660823\n",
      "Iteration 170, loss = 0.24563938\n",
      "Iteration 171, loss = 0.24471989\n",
      "Iteration 172, loss = 0.24372849\n",
      "Iteration 173, loss = 0.24275013\n",
      "Iteration 174, loss = 0.24178595\n",
      "Iteration 175, loss = 0.24084668\n",
      "Iteration 176, loss = 0.23987678\n",
      "Iteration 177, loss = 0.23886762\n",
      "Iteration 178, loss = 0.23782601\n",
      "Iteration 179, loss = 0.23678200\n",
      "Iteration 180, loss = 0.23565216\n",
      "Iteration 181, loss = 0.23458077\n",
      "Iteration 182, loss = 0.23345958\n",
      "Iteration 183, loss = 0.23234843\n",
      "Iteration 184, loss = 0.23123482\n",
      "Iteration 185, loss = 0.23016913\n",
      "Iteration 186, loss = 0.22904043\n",
      "Iteration 187, loss = 0.22795385\n",
      "Iteration 188, loss = 0.22683068\n",
      "Iteration 189, loss = 0.22564344\n",
      "Iteration 190, loss = 0.22449616\n",
      "Iteration 191, loss = 0.22332668\n",
      "Iteration 192, loss = 0.22211899\n",
      "Iteration 193, loss = 0.22092017\n",
      "Iteration 194, loss = 0.21968600\n",
      "Iteration 195, loss = 0.21838474\n",
      "Iteration 196, loss = 0.21707067\n",
      "Iteration 197, loss = 0.21580321\n",
      "Iteration 198, loss = 0.21450788\n",
      "Iteration 199, loss = 0.21325020\n",
      "Iteration 200, loss = 0.21199053\n",
      "Iteration 201, loss = 0.21069879\n",
      "Iteration 202, loss = 0.20938886\n",
      "Iteration 203, loss = 0.20808469\n",
      "Iteration 204, loss = 0.20680934\n",
      "Iteration 205, loss = 0.20549038\n",
      "Iteration 206, loss = 0.20413144\n",
      "Iteration 207, loss = 0.20270270\n",
      "Iteration 208, loss = 0.20126959\n",
      "Iteration 209, loss = 0.19989187\n",
      "Iteration 210, loss = 0.19846932\n",
      "Iteration 211, loss = 0.19707258\n",
      "Iteration 212, loss = 0.19569208\n",
      "Iteration 213, loss = 0.19430994\n",
      "Iteration 214, loss = 0.19282890\n",
      "Iteration 215, loss = 0.19144065\n",
      "Iteration 216, loss = 0.18999001\n",
      "Iteration 217, loss = 0.18850137\n",
      "Iteration 218, loss = 0.18696578\n",
      "Iteration 219, loss = 0.18543278\n",
      "Iteration 220, loss = 0.18397450\n",
      "Iteration 221, loss = 0.18250811\n",
      "Iteration 222, loss = 0.18104157\n",
      "Iteration 223, loss = 0.17951182\n",
      "Iteration 224, loss = 0.17793877\n",
      "Iteration 225, loss = 0.17638352\n",
      "Iteration 226, loss = 0.17480099\n",
      "Iteration 227, loss = 0.17320418\n",
      "Iteration 228, loss = 0.17149700\n",
      "Iteration 229, loss = 0.16987940\n",
      "Iteration 230, loss = 0.16814465\n",
      "Iteration 231, loss = 0.16655795\n",
      "Iteration 232, loss = 0.16492817\n",
      "Iteration 233, loss = 0.16340066\n",
      "Iteration 234, loss = 0.16189619\n",
      "Iteration 235, loss = 0.16046356\n",
      "Iteration 236, loss = 0.15901269\n",
      "Iteration 237, loss = 0.15759691\n",
      "Iteration 238, loss = 0.15612603\n",
      "Iteration 239, loss = 0.15469015\n",
      "Iteration 240, loss = 0.15324537\n",
      "Iteration 241, loss = 0.15186573\n",
      "Iteration 242, loss = 0.15056732\n",
      "Iteration 243, loss = 0.14942156\n",
      "Iteration 244, loss = 0.14832539\n",
      "Iteration 245, loss = 0.14717585\n",
      "Iteration 246, loss = 0.14606446\n",
      "Iteration 247, loss = 0.14497518\n",
      "Iteration 248, loss = 0.14388465\n",
      "Iteration 249, loss = 0.14287363\n",
      "Iteration 250, loss = 0.14183478\n",
      "Iteration 251, loss = 0.14085525\n",
      "Iteration 252, loss = 0.13986709\n",
      "Iteration 253, loss = 0.13883923\n",
      "Iteration 254, loss = 0.13788844\n",
      "Iteration 255, loss = 0.13692575\n",
      "Iteration 256, loss = 0.13599002\n",
      "Iteration 257, loss = 0.13504320\n",
      "Iteration 258, loss = 0.13412935\n",
      "Iteration 259, loss = 0.13341653\n",
      "Iteration 260, loss = 0.13274121\n",
      "Iteration 261, loss = 0.13211098\n",
      "Iteration 262, loss = 0.13151393\n",
      "Iteration 263, loss = 0.13088076\n",
      "Iteration 264, loss = 0.13028234\n",
      "Iteration 265, loss = 0.12970411\n",
      "Iteration 266, loss = 0.12913017\n",
      "Iteration 267, loss = 0.12857367\n",
      "Iteration 268, loss = 0.12800680\n",
      "Iteration 269, loss = 0.12747802\n",
      "Iteration 270, loss = 0.12694094\n",
      "Iteration 271, loss = 0.12640163\n",
      "Iteration 272, loss = 0.12589440\n",
      "Iteration 273, loss = 0.12534762\n",
      "Iteration 274, loss = 0.12483614\n",
      "Iteration 275, loss = 0.12432647\n",
      "Iteration 276, loss = 0.12382073\n",
      "Iteration 277, loss = 0.12333402\n",
      "Iteration 278, loss = 0.12282885\n",
      "Iteration 279, loss = 0.12235215\n",
      "Iteration 280, loss = 0.12187132\n",
      "Iteration 281, loss = 0.12138360\n",
      "Iteration 282, loss = 0.12089782\n",
      "Iteration 283, loss = 0.12036669\n",
      "Iteration 284, loss = 0.11971914\n",
      "Iteration 285, loss = 0.11902153\n",
      "Iteration 286, loss = 0.11834830\n",
      "Iteration 287, loss = 0.11769807\n",
      "Iteration 288, loss = 0.11703394\n",
      "Iteration 289, loss = 0.11646811\n",
      "Iteration 290, loss = 0.11583630\n",
      "Iteration 291, loss = 0.11523594\n",
      "Iteration 292, loss = 0.11469576\n",
      "Iteration 293, loss = 0.11411851\n",
      "Iteration 294, loss = 0.11357803\n",
      "Iteration 295, loss = 0.11301973\n",
      "Iteration 296, loss = 0.11250129\n",
      "Iteration 297, loss = 0.11197678\n",
      "Iteration 298, loss = 0.11144620\n",
      "Iteration 299, loss = 0.11094095\n",
      "Iteration 300, loss = 0.11047022\n",
      "Iteration 301, loss = 0.10997275\n",
      "Iteration 302, loss = 0.10950075\n",
      "Iteration 303, loss = 0.10904152\n",
      "Iteration 304, loss = 0.10860391\n",
      "Iteration 305, loss = 0.10815109\n",
      "Iteration 306, loss = 0.10771349\n",
      "Iteration 307, loss = 0.10730378\n",
      "Iteration 308, loss = 0.10686298\n",
      "Iteration 309, loss = 0.10644565\n",
      "Iteration 310, loss = 0.10603013\n",
      "Iteration 311, loss = 0.10561346\n",
      "Iteration 312, loss = 0.10520047\n",
      "Iteration 313, loss = 0.10480535\n",
      "Iteration 314, loss = 0.10441612\n",
      "Iteration 315, loss = 0.10401669\n",
      "Iteration 316, loss = 0.10363388\n",
      "Iteration 317, loss = 0.10323169\n",
      "Iteration 318, loss = 0.10286447\n",
      "Iteration 319, loss = 0.10248278\n",
      "Iteration 320, loss = 0.10210437\n",
      "Iteration 321, loss = 0.10172510\n",
      "Iteration 322, loss = 0.10134561\n",
      "Iteration 323, loss = 0.10099083\n",
      "Iteration 324, loss = 0.10059642\n",
      "Iteration 325, loss = 0.10024452\n",
      "Iteration 326, loss = 0.09988017\n",
      "Iteration 327, loss = 0.09951796\n",
      "Iteration 328, loss = 0.09918487\n",
      "Iteration 329, loss = 0.09879142\n",
      "Iteration 330, loss = 0.09844689\n",
      "Iteration 331, loss = 0.09809414\n",
      "Iteration 332, loss = 0.09774915\n",
      "Iteration 333, loss = 0.09741642\n",
      "Iteration 334, loss = 0.09703846\n",
      "Iteration 335, loss = 0.09670574\n",
      "Iteration 336, loss = 0.09636688\n",
      "Iteration 337, loss = 0.09602616\n",
      "Iteration 338, loss = 0.09570056\n",
      "Iteration 339, loss = 0.09537337\n",
      "Iteration 340, loss = 0.09504068\n",
      "Iteration 341, loss = 0.09472324\n",
      "Iteration 342, loss = 0.09439281\n",
      "Iteration 343, loss = 0.09406387\n",
      "Iteration 344, loss = 0.09373155\n",
      "Iteration 345, loss = 0.09342955\n",
      "Iteration 346, loss = 0.09311315\n",
      "Iteration 347, loss = 0.09278018\n",
      "Iteration 348, loss = 0.09246324\n",
      "Iteration 349, loss = 0.09216215\n",
      "Iteration 350, loss = 0.09185684\n",
      "Iteration 351, loss = 0.09154299\n",
      "Iteration 352, loss = 0.09123004\n",
      "Iteration 353, loss = 0.09092061\n",
      "Iteration 354, loss = 0.09062446\n",
      "Iteration 355, loss = 0.09033261\n",
      "Iteration 356, loss = 0.09002951\n",
      "Iteration 357, loss = 0.08971444\n",
      "Iteration 358, loss = 0.08940661\n",
      "Iteration 359, loss = 0.08912236\n",
      "Iteration 360, loss = 0.08881700\n",
      "Iteration 361, loss = 0.08851751\n",
      "Iteration 362, loss = 0.08820064\n",
      "Iteration 363, loss = 0.08789833\n",
      "Iteration 364, loss = 0.08761543\n",
      "Iteration 365, loss = 0.08732974\n",
      "Iteration 366, loss = 0.08703810\n",
      "Iteration 367, loss = 0.08675308\n",
      "Iteration 368, loss = 0.08645849\n",
      "Iteration 369, loss = 0.08617608\n",
      "Iteration 370, loss = 0.08590444\n",
      "Iteration 371, loss = 0.08561479\n",
      "Iteration 372, loss = 0.08535214\n",
      "Iteration 373, loss = 0.08508556\n",
      "Iteration 374, loss = 0.08478842\n",
      "Iteration 375, loss = 0.08452036\n",
      "Iteration 376, loss = 0.08425981\n",
      "Iteration 377, loss = 0.08397414\n",
      "Iteration 378, loss = 0.08370594\n",
      "Iteration 379, loss = 0.08342854\n",
      "Iteration 380, loss = 0.08315274\n",
      "Iteration 381, loss = 0.08289364\n",
      "Iteration 382, loss = 0.08262109\n",
      "Iteration 383, loss = 0.08236789\n",
      "Iteration 384, loss = 0.08215248\n",
      "Iteration 385, loss = 0.08183541\n",
      "Iteration 386, loss = 0.08156675\n",
      "Iteration 387, loss = 0.08130236\n",
      "Iteration 388, loss = 0.08103318\n",
      "Iteration 389, loss = 0.08078855\n",
      "Iteration 390, loss = 0.08052005\n",
      "Iteration 391, loss = 0.08025887\n",
      "Iteration 392, loss = 0.08000867\n",
      "Iteration 393, loss = 0.07975223\n",
      "Iteration 394, loss = 0.07950735\n",
      "Iteration 395, loss = 0.07926747\n",
      "Iteration 396, loss = 0.07901185\n",
      "Iteration 397, loss = 0.07874075\n",
      "Iteration 398, loss = 0.07849313\n",
      "Iteration 399, loss = 0.07826847\n",
      "Iteration 400, loss = 0.07800565\n",
      "Iteration 401, loss = 0.07779185\n",
      "Iteration 402, loss = 0.07753660\n",
      "Iteration 403, loss = 0.07726662\n",
      "Iteration 404, loss = 0.07703104\n",
      "Iteration 405, loss = 0.07676769\n",
      "Iteration 406, loss = 0.07652895\n",
      "Iteration 407, loss = 0.07630237\n",
      "Iteration 408, loss = 0.07604889\n",
      "Iteration 409, loss = 0.07580444\n",
      "Iteration 410, loss = 0.07555690\n",
      "Iteration 411, loss = 0.07533114\n",
      "Iteration 412, loss = 0.07508956\n",
      "Iteration 413, loss = 0.07484006\n",
      "Iteration 414, loss = 0.07460479\n",
      "Iteration 415, loss = 0.07435734\n",
      "Iteration 416, loss = 0.07417070\n",
      "Iteration 417, loss = 0.07389452\n",
      "Iteration 418, loss = 0.07367679\n",
      "Iteration 419, loss = 0.07344324\n",
      "Iteration 420, loss = 0.07319064\n",
      "Iteration 421, loss = 0.07296414\n",
      "Iteration 422, loss = 0.07273724\n",
      "Iteration 423, loss = 0.07250706\n",
      "Iteration 424, loss = 0.07232276\n",
      "Iteration 425, loss = 0.07205284\n",
      "Iteration 426, loss = 0.07182395\n",
      "Iteration 427, loss = 0.07158867\n",
      "Iteration 428, loss = 0.07138039\n",
      "Iteration 429, loss = 0.07111987\n",
      "Iteration 430, loss = 0.07090380\n",
      "Iteration 431, loss = 0.07070433\n",
      "Iteration 432, loss = 0.07045249\n",
      "Iteration 433, loss = 0.07024884\n",
      "Iteration 434, loss = 0.07000316\n",
      "Iteration 435, loss = 0.06977712\n",
      "Iteration 436, loss = 0.06953791\n",
      "Iteration 437, loss = 0.06928407\n",
      "Iteration 438, loss = 0.06906645\n",
      "Iteration 439, loss = 0.06884297\n",
      "Iteration 440, loss = 0.06861713\n",
      "Iteration 441, loss = 0.06841924\n",
      "Iteration 442, loss = 0.06816317\n",
      "Iteration 443, loss = 0.06794434\n",
      "Iteration 444, loss = 0.06771392\n",
      "Iteration 445, loss = 0.06750607\n",
      "Iteration 446, loss = 0.06728292\n",
      "Iteration 447, loss = 0.06707325\n",
      "Iteration 448, loss = 0.06683059\n",
      "Iteration 449, loss = 0.06662660\n",
      "Iteration 450, loss = 0.06640507\n",
      "Iteration 451, loss = 0.06618528\n",
      "Iteration 452, loss = 0.06598717\n",
      "Iteration 453, loss = 0.06580938\n",
      "Iteration 454, loss = 0.06553535\n",
      "Iteration 455, loss = 0.06531581\n",
      "Iteration 456, loss = 0.06510015\n",
      "Iteration 457, loss = 0.06488825\n",
      "Iteration 458, loss = 0.06467164\n",
      "Iteration 459, loss = 0.06445061\n",
      "Iteration 460, loss = 0.06425899\n",
      "Iteration 461, loss = 0.06403932\n",
      "Iteration 462, loss = 0.06383758\n",
      "Iteration 463, loss = 0.06363324\n",
      "Iteration 464, loss = 0.06343371\n",
      "Iteration 465, loss = 0.06321279\n",
      "Iteration 466, loss = 0.06299664\n",
      "Iteration 467, loss = 0.06278292\n",
      "Iteration 468, loss = 0.06257124\n",
      "Iteration 469, loss = 0.06236354\n",
      "Iteration 470, loss = 0.06216998\n",
      "Iteration 471, loss = 0.06196203\n",
      "Iteration 472, loss = 0.06175704\n",
      "Iteration 473, loss = 0.06155700\n",
      "Iteration 474, loss = 0.06136035\n",
      "Iteration 475, loss = 0.06113926\n",
      "Iteration 476, loss = 0.06095302\n",
      "Iteration 477, loss = 0.06073356\n",
      "Iteration 478, loss = 0.06051352\n",
      "Iteration 479, loss = 0.06034294\n",
      "Iteration 480, loss = 0.06012004\n",
      "Iteration 481, loss = 0.05991540\n",
      "Iteration 482, loss = 0.05971459\n",
      "Iteration 483, loss = 0.05951053\n",
      "Iteration 484, loss = 0.05931005\n",
      "Iteration 485, loss = 0.05915103\n",
      "Iteration 486, loss = 0.05889951\n",
      "Iteration 487, loss = 0.05871479\n",
      "Iteration 488, loss = 0.05852161\n",
      "Iteration 489, loss = 0.05832182\n",
      "Iteration 490, loss = 0.05813681\n",
      "Iteration 491, loss = 0.05791646\n",
      "Iteration 492, loss = 0.05774358\n",
      "Iteration 493, loss = 0.05756964\n",
      "Iteration 494, loss = 0.05734859\n",
      "Iteration 495, loss = 0.05713726\n",
      "Iteration 496, loss = 0.05696365\n",
      "Iteration 497, loss = 0.05677538\n",
      "Iteration 498, loss = 0.05654364\n",
      "Iteration 499, loss = 0.05636478\n",
      "Iteration 500, loss = 0.05616110\n",
      "Iteration 501, loss = 0.05597333\n",
      "Iteration 502, loss = 0.05578976\n",
      "Iteration 503, loss = 0.05562835\n",
      "Iteration 504, loss = 0.05540556\n",
      "Iteration 505, loss = 0.05522293\n",
      "Iteration 506, loss = 0.05501819\n",
      "Iteration 507, loss = 0.05484955\n",
      "Iteration 508, loss = 0.05464946\n",
      "Iteration 509, loss = 0.05446434\n",
      "Iteration 510, loss = 0.05429808\n",
      "Iteration 511, loss = 0.05409689\n",
      "Iteration 512, loss = 0.05392186\n",
      "Iteration 513, loss = 0.05372094\n",
      "Iteration 514, loss = 0.05355260\n",
      "Iteration 515, loss = 0.05335368\n",
      "Iteration 516, loss = 0.05322404\n",
      "Iteration 517, loss = 0.05303814\n",
      "Iteration 518, loss = 0.05283794\n",
      "Iteration 519, loss = 0.05262680\n",
      "Iteration 520, loss = 0.05245059\n",
      "Iteration 521, loss = 0.05227500\n",
      "Iteration 522, loss = 0.05211205\n",
      "Iteration 523, loss = 0.05191973\n",
      "Iteration 524, loss = 0.05173894\n",
      "Iteration 525, loss = 0.05157829\n",
      "Iteration 526, loss = 0.05138097\n",
      "Iteration 527, loss = 0.05120275\n",
      "Iteration 528, loss = 0.05104226\n",
      "Iteration 529, loss = 0.05088957\n",
      "Iteration 530, loss = 0.05068842\n",
      "Iteration 531, loss = 0.05051109\n",
      "Iteration 532, loss = 0.05032811\n",
      "Iteration 533, loss = 0.05016986\n",
      "Iteration 534, loss = 0.04999134\n",
      "Iteration 535, loss = 0.04982555\n",
      "Iteration 536, loss = 0.04963447\n",
      "Iteration 537, loss = 0.04948184\n",
      "Iteration 538, loss = 0.04930624\n",
      "Iteration 539, loss = 0.04913033\n",
      "Iteration 540, loss = 0.04896668\n",
      "Iteration 541, loss = 0.04882520\n",
      "Iteration 542, loss = 0.04862463\n",
      "Iteration 543, loss = 0.04845480\n",
      "Iteration 544, loss = 0.04828737\n",
      "Iteration 545, loss = 0.04812922\n",
      "Iteration 546, loss = 0.04796232\n",
      "Iteration 547, loss = 0.04776686\n",
      "Iteration 548, loss = 0.04760187\n",
      "Iteration 549, loss = 0.04744773\n",
      "Iteration 550, loss = 0.04728688\n",
      "Iteration 551, loss = 0.04713797\n",
      "Iteration 552, loss = 0.04695204\n",
      "Iteration 553, loss = 0.04680972\n",
      "Iteration 554, loss = 0.04663034\n",
      "Iteration 555, loss = 0.04646302\n",
      "Iteration 556, loss = 0.04630835\n",
      "Iteration 557, loss = 0.04615738\n",
      "Iteration 558, loss = 0.04598968\n",
      "Iteration 559, loss = 0.04583617\n",
      "Iteration 560, loss = 0.04567685\n",
      "Iteration 561, loss = 0.04553039\n",
      "Iteration 562, loss = 0.04535663\n",
      "Iteration 563, loss = 0.04521118\n",
      "Iteration 564, loss = 0.04504156\n",
      "Iteration 565, loss = 0.04486224\n",
      "Iteration 566, loss = 0.04472093\n",
      "Iteration 567, loss = 0.04455666\n",
      "Iteration 568, loss = 0.04440736\n",
      "Iteration 569, loss = 0.04423662\n",
      "Iteration 570, loss = 0.04408754\n",
      "Iteration 571, loss = 0.04392877\n",
      "Iteration 572, loss = 0.04375387\n",
      "Iteration 573, loss = 0.04360671\n",
      "Iteration 574, loss = 0.04345757\n",
      "Iteration 575, loss = 0.04329481\n",
      "Iteration 576, loss = 0.04312296\n",
      "Iteration 577, loss = 0.04297564\n",
      "Iteration 578, loss = 0.04283949\n",
      "Iteration 579, loss = 0.04265165\n",
      "Iteration 580, loss = 0.04249910\n",
      "Iteration 581, loss = 0.04235748\n",
      "Iteration 582, loss = 0.04218621\n",
      "Iteration 583, loss = 0.04204357\n",
      "Iteration 584, loss = 0.04193743\n",
      "Iteration 585, loss = 0.04174634\n",
      "Iteration 586, loss = 0.04166578\n",
      "Iteration 587, loss = 0.04145330\n",
      "Iteration 588, loss = 0.04133367\n",
      "Iteration 589, loss = 0.04118387\n",
      "Iteration 590, loss = 0.04102893\n",
      "Iteration 591, loss = 0.04089469\n",
      "Iteration 592, loss = 0.04074177\n",
      "Iteration 593, loss = 0.04062768\n",
      "Iteration 594, loss = 0.04046456\n",
      "Iteration 595, loss = 0.04033616\n",
      "Iteration 596, loss = 0.04020600\n",
      "Iteration 597, loss = 0.04004986\n",
      "Iteration 598, loss = 0.03992722\n",
      "Iteration 599, loss = 0.03978630\n",
      "Iteration 600, loss = 0.03965102\n",
      "Iteration 601, loss = 0.03953198\n",
      "Iteration 602, loss = 0.03938302\n",
      "Iteration 603, loss = 0.03922437\n",
      "Iteration 604, loss = 0.03908656\n",
      "Iteration 605, loss = 0.03895813\n",
      "Iteration 606, loss = 0.03885693\n",
      "Iteration 607, loss = 0.03869800\n",
      "Iteration 608, loss = 0.03855049\n",
      "Iteration 609, loss = 0.03841741\n",
      "Iteration 610, loss = 0.03828741\n",
      "Iteration 611, loss = 0.03816604\n",
      "Iteration 612, loss = 0.03802703\n",
      "Iteration 613, loss = 0.03791000\n",
      "Iteration 614, loss = 0.03777336\n",
      "Iteration 615, loss = 0.03764887\n",
      "Iteration 616, loss = 0.03750683\n",
      "Iteration 617, loss = 0.03740668\n",
      "Iteration 618, loss = 0.03727420\n",
      "Iteration 619, loss = 0.03714046\n",
      "Iteration 620, loss = 0.03705692\n",
      "Iteration 621, loss = 0.03688675\n",
      "Iteration 622, loss = 0.03676911\n",
      "Iteration 623, loss = 0.03666216\n",
      "Iteration 624, loss = 0.03652416\n",
      "Iteration 625, loss = 0.03639683\n",
      "Iteration 626, loss = 0.03626184\n",
      "Iteration 627, loss = 0.03614529\n",
      "Iteration 628, loss = 0.03602785\n",
      "Iteration 629, loss = 0.03590430\n",
      "Iteration 630, loss = 0.03579725\n",
      "Iteration 631, loss = 0.03567963\n",
      "Iteration 632, loss = 0.03555769\n",
      "Iteration 633, loss = 0.03543306\n",
      "Iteration 634, loss = 0.03529431\n",
      "Iteration 635, loss = 0.03516748\n",
      "Iteration 636, loss = 0.03506503\n",
      "Iteration 637, loss = 0.03493282\n",
      "Iteration 638, loss = 0.03481193\n",
      "Iteration 639, loss = 0.03472410\n",
      "Iteration 640, loss = 0.03461976\n",
      "Iteration 641, loss = 0.03447419\n",
      "Iteration 642, loss = 0.03434832\n",
      "Iteration 643, loss = 0.03424897\n",
      "Iteration 644, loss = 0.03413742\n",
      "Iteration 645, loss = 0.03401203\n",
      "Iteration 646, loss = 0.03391923\n",
      "Iteration 647, loss = 0.03382684\n",
      "Iteration 648, loss = 0.03368599\n",
      "Iteration 649, loss = 0.03356802\n",
      "Iteration 650, loss = 0.03346145\n",
      "Iteration 651, loss = 0.03334070\n",
      "Iteration 652, loss = 0.03321825\n",
      "Iteration 653, loss = 0.03311241\n",
      "Iteration 654, loss = 0.03300950\n",
      "Iteration 655, loss = 0.03288007\n",
      "Iteration 656, loss = 0.03277020\n",
      "Iteration 657, loss = 0.03267557\n",
      "Iteration 658, loss = 0.03256181\n",
      "Iteration 659, loss = 0.03244945\n",
      "Iteration 660, loss = 0.03233279\n",
      "Iteration 661, loss = 0.03223264\n",
      "Iteration 662, loss = 0.03212273\n",
      "Iteration 663, loss = 0.03202922\n",
      "Iteration 664, loss = 0.03190078\n",
      "Iteration 665, loss = 0.03180539\n",
      "Iteration 666, loss = 0.03167624\n",
      "Iteration 667, loss = 0.03158633\n",
      "Iteration 668, loss = 0.03149659\n",
      "Iteration 669, loss = 0.03137866\n",
      "Iteration 670, loss = 0.03126486\n",
      "Iteration 671, loss = 0.03117250\n",
      "Iteration 672, loss = 0.03108202\n",
      "Iteration 673, loss = 0.03095203\n",
      "Iteration 674, loss = 0.03084608\n",
      "Iteration 675, loss = 0.03075529\n",
      "Iteration 676, loss = 0.03066358\n",
      "Iteration 677, loss = 0.03054606\n",
      "Iteration 678, loss = 0.03044419\n",
      "Iteration 679, loss = 0.03035135\n",
      "Iteration 680, loss = 0.03023713\n",
      "Iteration 681, loss = 0.03013709\n",
      "Iteration 682, loss = 0.03003448\n",
      "Iteration 683, loss = 0.02995401\n",
      "Iteration 684, loss = 0.02985038\n",
      "Iteration 685, loss = 0.02973741\n",
      "Iteration 686, loss = 0.02961824\n",
      "Iteration 687, loss = 0.02955477\n",
      "Iteration 688, loss = 0.02944565\n",
      "Iteration 689, loss = 0.02936295\n",
      "Iteration 690, loss = 0.02924253\n",
      "Iteration 691, loss = 0.02913843\n",
      "Iteration 692, loss = 0.02906139\n",
      "Iteration 693, loss = 0.02896691\n",
      "Iteration 694, loss = 0.02885094\n",
      "Iteration 695, loss = 0.02876958\n",
      "Iteration 696, loss = 0.02867813\n",
      "Iteration 697, loss = 0.02857337\n",
      "Iteration 698, loss = 0.02846578\n",
      "Iteration 699, loss = 0.02840518\n",
      "Iteration 700, loss = 0.02829135\n",
      "Iteration 701, loss = 0.02820787\n",
      "Iteration 702, loss = 0.02811364\n",
      "Iteration 703, loss = 0.02802315\n",
      "Iteration 704, loss = 0.02791000\n",
      "Iteration 705, loss = 0.02782390\n",
      "Iteration 706, loss = 0.02774229\n",
      "Iteration 707, loss = 0.02766853\n",
      "Iteration 708, loss = 0.02757263\n",
      "Iteration 709, loss = 0.02746127\n",
      "Iteration 710, loss = 0.02736892\n",
      "Iteration 711, loss = 0.02729199\n",
      "Iteration 712, loss = 0.02720196\n",
      "Iteration 713, loss = 0.02713352\n",
      "Iteration 714, loss = 0.02702009\n",
      "Iteration 715, loss = 0.02691982\n",
      "Iteration 716, loss = 0.02685034\n",
      "Iteration 717, loss = 0.02675220\n",
      "Iteration 718, loss = 0.02667253\n",
      "Iteration 719, loss = 0.02659198\n",
      "Iteration 720, loss = 0.02648998\n",
      "Iteration 721, loss = 0.02640203\n",
      "Iteration 722, loss = 0.02631957\n",
      "Iteration 723, loss = 0.02623730\n",
      "Iteration 724, loss = 0.02614439\n",
      "Iteration 725, loss = 0.02604507\n",
      "Iteration 726, loss = 0.02597183\n",
      "Iteration 727, loss = 0.02587053\n",
      "Iteration 728, loss = 0.02578191\n",
      "Iteration 729, loss = 0.02569999\n",
      "Iteration 730, loss = 0.02564264\n",
      "Iteration 731, loss = 0.02554790\n",
      "Iteration 732, loss = 0.02544706\n",
      "Iteration 733, loss = 0.02537410\n",
      "Iteration 734, loss = 0.02529180\n",
      "Iteration 735, loss = 0.02518090\n",
      "Iteration 736, loss = 0.02513220\n",
      "Iteration 737, loss = 0.02503477\n",
      "Iteration 738, loss = 0.02494271\n",
      "Iteration 739, loss = 0.02486032\n",
      "Iteration 740, loss = 0.02478093\n",
      "Iteration 741, loss = 0.02472768\n",
      "Iteration 742, loss = 0.02460969\n",
      "Iteration 743, loss = 0.02454251\n",
      "Iteration 744, loss = 0.02447626\n",
      "Iteration 745, loss = 0.02436374\n",
      "Iteration 746, loss = 0.02427985\n",
      "Iteration 747, loss = 0.02420882\n",
      "Iteration 748, loss = 0.02412293\n",
      "Iteration 749, loss = 0.02407182\n",
      "Iteration 750, loss = 0.02397334\n",
      "Iteration 751, loss = 0.02389848\n",
      "Iteration 752, loss = 0.02384092\n",
      "Iteration 753, loss = 0.02374523\n",
      "Iteration 754, loss = 0.02364613\n",
      "Iteration 755, loss = 0.02358539\n",
      "Iteration 756, loss = 0.02351383\n",
      "Iteration 757, loss = 0.02344722\n",
      "Iteration 758, loss = 0.02340902\n",
      "Iteration 759, loss = 0.02326071\n",
      "Iteration 760, loss = 0.02319364\n",
      "Iteration 761, loss = 0.02310820\n",
      "Iteration 762, loss = 0.02304440\n",
      "Iteration 763, loss = 0.02298083\n",
      "Iteration 764, loss = 0.02286931\n",
      "Iteration 765, loss = 0.02282768\n",
      "Iteration 766, loss = 0.02275446\n",
      "Iteration 767, loss = 0.02267070\n",
      "Iteration 768, loss = 0.02261681\n",
      "Iteration 769, loss = 0.02250478\n",
      "Iteration 770, loss = 0.02242960\n",
      "Iteration 771, loss = 0.02235942\n",
      "Iteration 772, loss = 0.02227417\n",
      "Iteration 773, loss = 0.02221061\n",
      "Iteration 774, loss = 0.02213012\n",
      "Iteration 775, loss = 0.02206201\n",
      "Iteration 776, loss = 0.02198697\n",
      "Iteration 777, loss = 0.02192093\n",
      "Iteration 778, loss = 0.02187836\n",
      "Iteration 779, loss = 0.02177923\n",
      "Iteration 780, loss = 0.02172285\n",
      "Iteration 781, loss = 0.02163125\n",
      "Iteration 782, loss = 0.02157611\n",
      "Iteration 783, loss = 0.02149073\n",
      "Iteration 784, loss = 0.02142738\n",
      "Iteration 785, loss = 0.02138424\n",
      "Iteration 786, loss = 0.02130651\n",
      "Iteration 787, loss = 0.02124257\n",
      "Iteration 788, loss = 0.02114627\n",
      "Iteration 789, loss = 0.02109185\n",
      "Iteration 790, loss = 0.02100353\n",
      "Iteration 791, loss = 0.02093862\n",
      "Iteration 792, loss = 0.02087600\n",
      "Iteration 793, loss = 0.02080758\n",
      "Iteration 794, loss = 0.02073969\n",
      "Iteration 795, loss = 0.02065644\n",
      "Iteration 796, loss = 0.02060808\n",
      "Iteration 797, loss = 0.02054115\n",
      "Iteration 798, loss = 0.02048616\n",
      "Iteration 799, loss = 0.02038643\n",
      "Iteration 800, loss = 0.02034005\n",
      "Iteration 801, loss = 0.02028241\n",
      "Iteration 802, loss = 0.02020291\n",
      "Iteration 803, loss = 0.02019082\n",
      "Iteration 804, loss = 0.02008624\n",
      "Iteration 805, loss = 0.02000802\n",
      "Iteration 806, loss = 0.01994789\n",
      "Iteration 807, loss = 0.01989334\n",
      "Iteration 808, loss = 0.01983025\n",
      "Iteration 809, loss = 0.01976436\n",
      "Iteration 810, loss = 0.01968358\n",
      "Iteration 811, loss = 0.01962113\n",
      "Iteration 812, loss = 0.01956716\n",
      "Iteration 813, loss = 0.01948717\n",
      "Iteration 814, loss = 0.01942861\n",
      "Iteration 815, loss = 0.01938484\n",
      "Iteration 816, loss = 0.01930572\n",
      "Iteration 817, loss = 0.01923875\n",
      "Iteration 818, loss = 0.01919012\n",
      "Iteration 819, loss = 0.01915035\n",
      "Iteration 820, loss = 0.01905737\n",
      "Iteration 821, loss = 0.01899279\n",
      "Iteration 822, loss = 0.01894986\n",
      "Iteration 823, loss = 0.01888614\n",
      "Iteration 824, loss = 0.01882339\n",
      "Iteration 825, loss = 0.01880040\n",
      "Iteration 826, loss = 0.01870542\n",
      "Iteration 827, loss = 0.01863910\n",
      "Iteration 828, loss = 0.01859024\n",
      "Iteration 829, loss = 0.01851680\n",
      "Iteration 830, loss = 0.01845055\n",
      "Iteration 831, loss = 0.01839109\n",
      "Iteration 832, loss = 0.01832924\n",
      "Iteration 833, loss = 0.01826951\n",
      "Iteration 834, loss = 0.01820803\n",
      "Iteration 835, loss = 0.01815249\n",
      "Iteration 836, loss = 0.01811080\n",
      "Iteration 837, loss = 0.01804858\n",
      "Iteration 838, loss = 0.01800911\n",
      "Iteration 839, loss = 0.01796052\n",
      "Iteration 840, loss = 0.01787113\n",
      "Iteration 841, loss = 0.01780402\n",
      "Iteration 842, loss = 0.01776722\n",
      "Iteration 843, loss = 0.01771553\n",
      "Iteration 844, loss = 0.01764931\n",
      "Iteration 845, loss = 0.01758206\n",
      "Iteration 846, loss = 0.01754270\n",
      "Iteration 847, loss = 0.01747027\n",
      "Iteration 848, loss = 0.01742870\n",
      "Iteration 849, loss = 0.01737942\n",
      "Iteration 850, loss = 0.01731027\n",
      "Iteration 851, loss = 0.01727365\n",
      "Iteration 852, loss = 0.01718397\n",
      "Iteration 853, loss = 0.01713926\n",
      "Iteration 854, loss = 0.01709553\n",
      "Iteration 855, loss = 0.01704481\n",
      "Iteration 856, loss = 0.01697224\n",
      "Iteration 857, loss = 0.01692356\n",
      "Iteration 858, loss = 0.01686179\n",
      "Iteration 859, loss = 0.01681713\n",
      "Iteration 860, loss = 0.01676105\n",
      "Iteration 861, loss = 0.01669042\n",
      "Iteration 862, loss = 0.01665002\n",
      "Iteration 863, loss = 0.01659655\n",
      "Iteration 864, loss = 0.01656744\n",
      "Iteration 865, loss = 0.01648179\n",
      "Iteration 866, loss = 0.01643772\n",
      "Iteration 867, loss = 0.01640843\n",
      "Iteration 868, loss = 0.01634110\n",
      "Iteration 869, loss = 0.01628793\n",
      "Iteration 870, loss = 0.01621972\n",
      "Iteration 871, loss = 0.01616916\n",
      "Iteration 872, loss = 0.01612141\n",
      "Iteration 873, loss = 0.01612421\n",
      "Iteration 874, loss = 0.01604555\n",
      "Iteration 875, loss = 0.01600127\n",
      "Iteration 876, loss = 0.01590565\n",
      "Iteration 877, loss = 0.01586823\n",
      "Iteration 878, loss = 0.01581072\n",
      "Iteration 879, loss = 0.01578726\n",
      "Iteration 880, loss = 0.01571145\n",
      "Iteration 881, loss = 0.01565061\n",
      "Iteration 882, loss = 0.01563916\n",
      "Iteration 883, loss = 0.01556779\n",
      "Iteration 884, loss = 0.01557315\n",
      "Iteration 885, loss = 0.01546722\n",
      "Iteration 886, loss = 0.01542469\n",
      "Iteration 887, loss = 0.01538332\n",
      "Iteration 888, loss = 0.01532609\n",
      "Iteration 889, loss = 0.01527879\n",
      "Iteration 890, loss = 0.01522485\n",
      "Iteration 891, loss = 0.01517451\n",
      "Iteration 892, loss = 0.01514306\n",
      "Iteration 893, loss = 0.01508215\n",
      "Iteration 894, loss = 0.01504728\n",
      "Iteration 895, loss = 0.01499032\n",
      "Iteration 896, loss = 0.01495176\n",
      "Iteration 897, loss = 0.01488386\n",
      "Iteration 898, loss = 0.01485587\n",
      "Iteration 899, loss = 0.01478847\n",
      "Iteration 900, loss = 0.01475048\n",
      "Iteration 901, loss = 0.01470177\n",
      "Iteration 902, loss = 0.01464644\n",
      "Iteration 903, loss = 0.01462562\n",
      "Iteration 904, loss = 0.01456296\n",
      "Iteration 905, loss = 0.01454000\n",
      "Iteration 906, loss = 0.01448908\n",
      "Iteration 907, loss = 0.01441711\n",
      "Iteration 908, loss = 0.01437233\n",
      "Iteration 909, loss = 0.01433549\n",
      "Iteration 910, loss = 0.01428791\n",
      "Iteration 911, loss = 0.01426468\n",
      "Iteration 912, loss = 0.01420506\n",
      "Iteration 913, loss = 0.01416789\n",
      "Iteration 914, loss = 0.01412185\n",
      "Iteration 915, loss = 0.01407437\n",
      "Iteration 916, loss = 0.01403225\n",
      "Iteration 917, loss = 0.01398834\n",
      "Iteration 918, loss = 0.01396444\n",
      "Iteration 919, loss = 0.01389960\n",
      "Iteration 920, loss = 0.01385785\n",
      "Iteration 921, loss = 0.01380912\n",
      "Iteration 922, loss = 0.01378004\n",
      "Iteration 923, loss = 0.01371777\n",
      "Iteration 924, loss = 0.01370135\n",
      "Iteration 925, loss = 0.01363081\n",
      "Iteration 926, loss = 0.01358669\n",
      "Iteration 927, loss = 0.01355656\n",
      "Iteration 928, loss = 0.01349839\n",
      "Iteration 929, loss = 0.01348480\n",
      "Iteration 930, loss = 0.01346050\n",
      "Iteration 931, loss = 0.01339571\n",
      "Iteration 932, loss = 0.01334082\n",
      "Iteration 933, loss = 0.01330421\n",
      "Iteration 934, loss = 0.01326501\n",
      "Iteration 935, loss = 0.01322361\n",
      "Iteration 936, loss = 0.01317129\n",
      "Iteration 937, loss = 0.01315362\n",
      "Iteration 938, loss = 0.01308973\n",
      "Iteration 939, loss = 0.01305864\n",
      "Iteration 940, loss = 0.01302439\n",
      "Iteration 941, loss = 0.01297880\n",
      "Iteration 942, loss = 0.01295679\n",
      "Iteration 943, loss = 0.01289563\n",
      "Iteration 944, loss = 0.01284545\n",
      "Iteration 945, loss = 0.01280264\n",
      "Iteration 946, loss = 0.01275492\n",
      "Iteration 947, loss = 0.01276393\n",
      "Iteration 948, loss = 0.01270475\n",
      "Iteration 949, loss = 0.01264559\n",
      "Iteration 950, loss = 0.01261601\n",
      "Iteration 951, loss = 0.01257160\n",
      "Iteration 952, loss = 0.01255199\n",
      "Iteration 953, loss = 0.01250321\n",
      "Iteration 954, loss = 0.01246775\n",
      "Iteration 955, loss = 0.01241572\n",
      "Iteration 956, loss = 0.01237386\n",
      "Iteration 957, loss = 0.01234901\n",
      "Iteration 958, loss = 0.01230010\n",
      "Iteration 959, loss = 0.01227167\n",
      "Iteration 960, loss = 0.01222864\n",
      "Iteration 961, loss = 0.01221825\n",
      "Iteration 962, loss = 0.01217937\n",
      "Iteration 963, loss = 0.01212403\n",
      "Iteration 964, loss = 0.01209389\n",
      "Iteration 965, loss = 0.01203827\n",
      "Iteration 966, loss = 0.01202420\n",
      "Iteration 967, loss = 0.01197431\n",
      "Iteration 968, loss = 0.01192371\n",
      "Iteration 969, loss = 0.01189288\n",
      "Iteration 970, loss = 0.01184378\n",
      "Iteration 971, loss = 0.01183373\n",
      "Iteration 972, loss = 0.01176798\n",
      "Iteration 973, loss = 0.01177435\n",
      "Iteration 974, loss = 0.01168957\n",
      "Iteration 975, loss = 0.01170386\n",
      "Iteration 976, loss = 0.01163211\n",
      "Iteration 977, loss = 0.01160730\n",
      "Iteration 978, loss = 0.01156967\n",
      "Iteration 979, loss = 0.01155058\n",
      "Iteration 980, loss = 0.01148627\n",
      "Iteration 981, loss = 0.01146541\n",
      "Iteration 982, loss = 0.01144304\n",
      "Iteration 983, loss = 0.01140654\n",
      "Iteration 984, loss = 0.01136578\n",
      "Iteration 985, loss = 0.01133574\n",
      "Iteration 986, loss = 0.01127459\n",
      "Iteration 987, loss = 0.01127224\n",
      "Iteration 988, loss = 0.01122452\n",
      "Iteration 989, loss = 0.01119107\n",
      "Iteration 990, loss = 0.01114958\n",
      "Iteration 991, loss = 0.01112840\n",
      "Iteration 992, loss = 0.01109339\n",
      "Iteration 993, loss = 0.01106141\n",
      "Iteration 994, loss = 0.01100525\n",
      "Iteration 995, loss = 0.01097184\n",
      "Iteration 996, loss = 0.01095255\n",
      "Iteration 997, loss = 0.01090451\n",
      "Iteration 998, loss = 0.01089310\n",
      "Iteration 999, loss = 0.01083844\n",
      "Iteration 1000, loss = 0.01081539\n",
      "Iteration 1001, loss = 0.01076227\n",
      "Iteration 1002, loss = 0.01077748\n",
      "Iteration 1003, loss = 0.01071141\n",
      "Iteration 1004, loss = 0.01069614\n",
      "Iteration 1005, loss = 0.01064960\n",
      "Iteration 1006, loss = 0.01062290\n",
      "Iteration 1007, loss = 0.01057566\n",
      "Iteration 1008, loss = 0.01054640\n",
      "Iteration 1009, loss = 0.01051281\n",
      "Iteration 1010, loss = 0.01049604\n",
      "Iteration 1011, loss = 0.01047760\n",
      "Iteration 1012, loss = 0.01043805\n",
      "Iteration 1013, loss = 0.01039722\n",
      "Iteration 1014, loss = 0.01036763\n",
      "Iteration 1015, loss = 0.01035331\n",
      "Iteration 1016, loss = 0.01029910\n",
      "Iteration 1017, loss = 0.01025740\n",
      "Iteration 1018, loss = 0.01023558\n",
      "Iteration 1019, loss = 0.01021181\n",
      "Iteration 1020, loss = 0.01016850\n",
      "Iteration 1021, loss = 0.01013975\n",
      "Iteration 1022, loss = 0.01012072\n",
      "Iteration 1023, loss = 0.01008162\n",
      "Iteration 1024, loss = 0.01004240\n",
      "Iteration 1025, loss = 0.01000944\n",
      "Iteration 1026, loss = 0.01000220\n",
      "Iteration 1027, loss = 0.00995118\n",
      "Iteration 1028, loss = 0.00992459\n",
      "Iteration 1029, loss = 0.00991397\n",
      "Iteration 1030, loss = 0.00986885\n",
      "Iteration 1031, loss = 0.00982589\n",
      "Iteration 1032, loss = 0.00980257\n",
      "Iteration 1033, loss = 0.00980811\n",
      "Iteration 1034, loss = 0.00977285\n",
      "Iteration 1035, loss = 0.00973278\n",
      "Iteration 1036, loss = 0.00970935\n",
      "Iteration 1037, loss = 0.00965844\n",
      "Iteration 1038, loss = 0.00963971\n",
      "Iteration 1039, loss = 0.00962790\n",
      "Iteration 1040, loss = 0.00957204\n",
      "Iteration 1041, loss = 0.00955949\n",
      "Iteration 1042, loss = 0.00953528\n",
      "Iteration 1043, loss = 0.00948904\n",
      "Iteration 1044, loss = 0.00949133\n",
      "Iteration 1045, loss = 0.00942837\n",
      "Iteration 1046, loss = 0.00941826\n",
      "Iteration 1047, loss = 0.00937960\n",
      "Iteration 1048, loss = 0.00934026\n",
      "Iteration 1049, loss = 0.00931453\n",
      "Iteration 1050, loss = 0.00930857\n",
      "Iteration 1051, loss = 0.00927250\n",
      "Iteration 1052, loss = 0.00923941\n",
      "Iteration 1053, loss = 0.00919917\n",
      "Iteration 1054, loss = 0.00918111\n",
      "Iteration 1055, loss = 0.00916403\n",
      "Iteration 1056, loss = 0.00913535\n",
      "Iteration 1057, loss = 0.00911151\n",
      "Iteration 1058, loss = 0.00910357\n",
      "Iteration 1059, loss = 0.00906973\n",
      "Iteration 1060, loss = 0.00902791\n",
      "Iteration 1061, loss = 0.00897047\n",
      "Iteration 1062, loss = 0.00897593\n",
      "Iteration 1063, loss = 0.00897221\n",
      "Iteration 1064, loss = 0.00897154\n",
      "Iteration 1065, loss = 0.00890346\n",
      "Iteration 1066, loss = 0.00889905\n",
      "Iteration 1067, loss = 0.00887338\n",
      "Iteration 1068, loss = 0.00883230\n",
      "Iteration 1069, loss = 0.00878996\n",
      "Iteration 1070, loss = 0.00876411\n",
      "Iteration 1071, loss = 0.00875677\n",
      "Iteration 1072, loss = 0.00870936\n",
      "Iteration 1073, loss = 0.00869059\n",
      "Iteration 1074, loss = 0.00864465\n",
      "Iteration 1075, loss = 0.00863108\n",
      "Iteration 1076, loss = 0.00861048\n",
      "Iteration 1077, loss = 0.00859541\n",
      "Iteration 1078, loss = 0.00855286\n",
      "Iteration 1079, loss = 0.00852564\n",
      "Iteration 1080, loss = 0.00850663\n",
      "Iteration 1081, loss = 0.00846982\n",
      "Iteration 1082, loss = 0.00846792\n",
      "Iteration 1083, loss = 0.00847083\n",
      "Iteration 1084, loss = 0.00841521\n",
      "Iteration 1085, loss = 0.00837590\n",
      "Iteration 1086, loss = 0.00835153\n",
      "Iteration 1087, loss = 0.00834565\n",
      "Iteration 1088, loss = 0.00833127\n",
      "Iteration 1089, loss = 0.00828787\n",
      "Iteration 1090, loss = 0.00826460\n",
      "Iteration 1091, loss = 0.00823380\n",
      "Iteration 1092, loss = 0.00824461\n",
      "Iteration 1093, loss = 0.00819703\n",
      "Iteration 1094, loss = 0.00818622\n",
      "Iteration 1095, loss = 0.00816829\n",
      "Iteration 1096, loss = 0.00814418\n",
      "Iteration 1097, loss = 0.00810365\n",
      "Iteration 1098, loss = 0.00806860\n",
      "Iteration 1099, loss = 0.00805376\n",
      "Iteration 1100, loss = 0.00802506\n",
      "Iteration 1101, loss = 0.00799664\n",
      "Iteration 1102, loss = 0.00800403\n",
      "Iteration 1103, loss = 0.00797571\n",
      "Iteration 1104, loss = 0.00794224\n",
      "Iteration 1105, loss = 0.00790378\n",
      "Iteration 1106, loss = 0.00788827\n",
      "Iteration 1107, loss = 0.00787549\n",
      "Iteration 1108, loss = 0.00784655\n",
      "Iteration 1109, loss = 0.00784022\n",
      "Iteration 1110, loss = 0.00778715\n",
      "Iteration 1111, loss = 0.00776866\n",
      "Iteration 1112, loss = 0.00776372\n",
      "Iteration 1113, loss = 0.00773456\n",
      "Iteration 1114, loss = 0.00770188\n",
      "Iteration 1115, loss = 0.00768794\n",
      "Iteration 1116, loss = 0.00768994\n",
      "Iteration 1117, loss = 0.00766168\n",
      "Iteration 1118, loss = 0.00762047\n",
      "Iteration 1119, loss = 0.00762874\n",
      "Iteration 1120, loss = 0.00757580\n",
      "Iteration 1121, loss = 0.00757807\n",
      "Iteration 1122, loss = 0.00752916\n",
      "Iteration 1123, loss = 0.00755678\n",
      "Iteration 1124, loss = 0.00750201\n",
      "Iteration 1125, loss = 0.00748359\n",
      "Iteration 1126, loss = 0.00744789\n",
      "Iteration 1127, loss = 0.00742390\n",
      "Iteration 1128, loss = 0.00740947\n",
      "Iteration 1129, loss = 0.00740964\n",
      "Iteration 1130, loss = 0.00736248\n",
      "Iteration 1131, loss = 0.00734263\n",
      "Iteration 1132, loss = 0.00731588\n",
      "Iteration 1133, loss = 0.00729360\n",
      "Iteration 1134, loss = 0.00729167\n",
      "Iteration 1135, loss = 0.00726806\n",
      "Iteration 1136, loss = 0.00727324\n",
      "Iteration 1137, loss = 0.00724965\n",
      "Iteration 1138, loss = 0.00719865\n",
      "Iteration 1139, loss = 0.00717198\n",
      "Iteration 1140, loss = 0.00716352\n",
      "Iteration 1141, loss = 0.00712885\n",
      "Iteration 1142, loss = 0.00710887\n",
      "Iteration 1143, loss = 0.00711157\n",
      "Iteration 1144, loss = 0.00707972\n",
      "Iteration 1145, loss = 0.00706245\n",
      "Iteration 1146, loss = 0.00705006\n",
      "Iteration 1147, loss = 0.00702794\n",
      "Iteration 1148, loss = 0.00699985\n",
      "Iteration 1149, loss = 0.00699058\n",
      "Iteration 1150, loss = 0.00696206\n",
      "Iteration 1151, loss = 0.00696039\n",
      "Iteration 1152, loss = 0.00693266\n",
      "Iteration 1153, loss = 0.00692703\n",
      "Iteration 1154, loss = 0.00688529\n",
      "Iteration 1155, loss = 0.00686057\n",
      "Iteration 1156, loss = 0.00686315\n",
      "Iteration 1157, loss = 0.00681726\n",
      "Iteration 1158, loss = 0.00680686\n",
      "Iteration 1159, loss = 0.00679213\n",
      "Iteration 1160, loss = 0.00677083\n",
      "Iteration 1161, loss = 0.00673685\n",
      "Iteration 1162, loss = 0.00673198\n",
      "Iteration 1163, loss = 0.00670070\n",
      "Iteration 1164, loss = 0.00669822\n",
      "Iteration 1165, loss = 0.00667423\n",
      "Iteration 1166, loss = 0.00666217\n",
      "Iteration 1167, loss = 0.00663935\n",
      "Iteration 1168, loss = 0.00661814\n",
      "Iteration 1169, loss = 0.00662549\n",
      "Iteration 1170, loss = 0.00658580\n",
      "Iteration 1171, loss = 0.00657107\n",
      "Iteration 1172, loss = 0.00653329\n",
      "Iteration 1173, loss = 0.00653534\n",
      "Iteration 1174, loss = 0.00650573\n",
      "Iteration 1175, loss = 0.00649351\n",
      "Iteration 1176, loss = 0.00645425\n",
      "Iteration 1177, loss = 0.00647320\n",
      "Iteration 1178, loss = 0.00643926\n",
      "Iteration 1179, loss = 0.00643020\n",
      "Iteration 1180, loss = 0.00640860\n",
      "Iteration 1181, loss = 0.00638694\n",
      "Iteration 1182, loss = 0.00638656\n",
      "Iteration 1183, loss = 0.00635539\n",
      "Iteration 1184, loss = 0.00631799\n",
      "Iteration 1185, loss = 0.00633632\n",
      "Iteration 1186, loss = 0.00629589\n",
      "Iteration 1187, loss = 0.00627104\n",
      "Iteration 1188, loss = 0.00626028\n",
      "Iteration 1189, loss = 0.00626900\n",
      "Iteration 1190, loss = 0.00624077\n",
      "Iteration 1191, loss = 0.00620488\n",
      "Iteration 1192, loss = 0.00621804\n",
      "Iteration 1193, loss = 0.00618800\n",
      "Iteration 1194, loss = 0.00618477\n",
      "Iteration 1195, loss = 0.00614785\n",
      "Iteration 1196, loss = 0.00615650\n",
      "Iteration 1197, loss = 0.00610119\n",
      "Iteration 1198, loss = 0.00609996\n",
      "Iteration 1199, loss = 0.00607544\n",
      "Iteration 1200, loss = 0.00607329\n",
      "Iteration 1201, loss = 0.00603483\n",
      "Iteration 1202, loss = 0.00602805\n",
      "Iteration 1203, loss = 0.00601207\n",
      "Iteration 1204, loss = 0.00601689\n",
      "Iteration 1205, loss = 0.00596700\n",
      "Iteration 1206, loss = 0.00595520\n",
      "Iteration 1207, loss = 0.00596259\n",
      "Iteration 1208, loss = 0.00594884\n",
      "Iteration 1209, loss = 0.00591931\n",
      "Iteration 1210, loss = 0.00589794\n",
      "Iteration 1211, loss = 0.00588619\n",
      "Iteration 1212, loss = 0.00586442\n",
      "Iteration 1213, loss = 0.00587376\n",
      "Iteration 1214, loss = 0.00583906\n",
      "Iteration 1215, loss = 0.00582802\n",
      "Iteration 1216, loss = 0.00579746\n",
      "Iteration 1217, loss = 0.00577291\n",
      "Iteration 1218, loss = 0.00575701\n",
      "Iteration 1219, loss = 0.00574741\n",
      "Iteration 1220, loss = 0.00575574\n",
      "Iteration 1221, loss = 0.00572720\n",
      "Iteration 1222, loss = 0.00572089\n",
      "Iteration 1223, loss = 0.00567890\n",
      "Iteration 1224, loss = 0.00568409\n",
      "Iteration 1225, loss = 0.00564340\n",
      "Iteration 1226, loss = 0.00565738\n",
      "Iteration 1227, loss = 0.00562334\n",
      "Iteration 1228, loss = 0.00560140\n",
      "Iteration 1229, loss = 0.00558830\n",
      "Iteration 1230, loss = 0.00558960\n",
      "Iteration 1231, loss = 0.00556711\n",
      "Iteration 1232, loss = 0.00557425\n",
      "Iteration 1233, loss = 0.00553628\n",
      "Iteration 1234, loss = 0.00551481\n",
      "Iteration 1235, loss = 0.00549008\n",
      "Iteration 1236, loss = 0.00551391\n",
      "Iteration 1237, loss = 0.00548881\n",
      "Iteration 1238, loss = 0.00548398\n",
      "Iteration 1239, loss = 0.00544599\n",
      "Iteration 1240, loss = 0.00547379\n",
      "Iteration 1241, loss = 0.00541208\n",
      "Iteration 1242, loss = 0.00540767\n",
      "Iteration 1243, loss = 0.00539714\n",
      "Iteration 1244, loss = 0.00536746\n",
      "Iteration 1245, loss = 0.00538279\n",
      "Iteration 1246, loss = 0.00537963\n",
      "Iteration 1247, loss = 0.00532410\n",
      "Iteration 1248, loss = 0.00529857\n",
      "Iteration 1249, loss = 0.00529498\n",
      "Iteration 1250, loss = 0.00528146\n",
      "Iteration 1251, loss = 0.00528116\n",
      "Iteration 1252, loss = 0.00527355\n",
      "Iteration 1253, loss = 0.00523989\n",
      "Iteration 1254, loss = 0.00524074\n",
      "Iteration 1255, loss = 0.00521221\n",
      "Iteration 1256, loss = 0.00521663\n",
      "Iteration 1257, loss = 0.00521258\n",
      "Iteration 1258, loss = 0.00517554\n",
      "Iteration 1259, loss = 0.00515521\n",
      "Iteration 1260, loss = 0.00515104\n",
      "Iteration 1261, loss = 0.00511876\n",
      "Iteration 1262, loss = 0.00511113\n",
      "Iteration 1263, loss = 0.00511854\n",
      "Iteration 1264, loss = 0.00509776\n",
      "Iteration 1265, loss = 0.00507455\n",
      "Iteration 1266, loss = 0.00505624\n",
      "Iteration 1267, loss = 0.00506455\n",
      "Iteration 1268, loss = 0.00505239\n",
      "Iteration 1269, loss = 0.00501876\n",
      "Iteration 1270, loss = 0.00501010\n",
      "Iteration 1271, loss = 0.00501082\n",
      "Iteration 1272, loss = 0.00497873\n",
      "Iteration 1273, loss = 0.00497472\n",
      "Iteration 1274, loss = 0.00495658\n",
      "Iteration 1275, loss = 0.00494938\n",
      "Iteration 1276, loss = 0.00493873\n",
      "Iteration 1277, loss = 0.00493410\n",
      "Iteration 1278, loss = 0.00490056\n",
      "Iteration 1279, loss = 0.00491221\n",
      "Iteration 1280, loss = 0.00490023\n",
      "Iteration 1281, loss = 0.00490683\n",
      "Iteration 1282, loss = 0.00485179\n",
      "Iteration 1283, loss = 0.00483917\n",
      "Iteration 1284, loss = 0.00485895\n",
      "Iteration 1285, loss = 0.00481529\n",
      "Iteration 1286, loss = 0.00479917\n",
      "Iteration 1287, loss = 0.00477754\n",
      "Iteration 1288, loss = 0.00480287\n",
      "Iteration 1289, loss = 0.00478605\n",
      "Iteration 1290, loss = 0.00475855\n",
      "Iteration 1291, loss = 0.00473682\n",
      "Iteration 1292, loss = 0.00472506\n",
      "Iteration 1293, loss = 0.00471798\n",
      "Iteration 1294, loss = 0.00472121\n",
      "Iteration 1295, loss = 0.00472067\n",
      "Iteration 1296, loss = 0.00471800\n",
      "Iteration 1297, loss = 0.00465654\n",
      "Iteration 1298, loss = 0.00464990\n",
      "Iteration 1299, loss = 0.00463511\n",
      "Iteration 1300, loss = 0.00465753\n",
      "Iteration 1301, loss = 0.00462043\n",
      "Iteration 1302, loss = 0.00460953\n",
      "Iteration 1303, loss = 0.00459684\n",
      "Iteration 1304, loss = 0.00458296\n",
      "Iteration 1305, loss = 0.00456645\n",
      "Iteration 1306, loss = 0.00457565\n",
      "Iteration 1307, loss = 0.00455890\n",
      "Iteration 1308, loss = 0.00454596\n",
      "Iteration 1309, loss = 0.00453256\n",
      "Iteration 1310, loss = 0.00451783\n",
      "Iteration 1311, loss = 0.00451626\n",
      "Iteration 1312, loss = 0.00449390\n",
      "Iteration 1313, loss = 0.00449452\n",
      "Iteration 1314, loss = 0.00446477\n",
      "Iteration 1315, loss = 0.00445140\n",
      "Iteration 1316, loss = 0.00445934\n",
      "Iteration 1317, loss = 0.00443632\n",
      "Iteration 1318, loss = 0.00441892\n",
      "Iteration 1319, loss = 0.00440391\n",
      "Iteration 1320, loss = 0.00442261\n",
      "Iteration 1321, loss = 0.00437515\n",
      "Iteration 1322, loss = 0.00437243\n",
      "Iteration 1323, loss = 0.00438446\n",
      "Iteration 1324, loss = 0.00435803\n",
      "Iteration 1325, loss = 0.00434069\n",
      "Iteration 1326, loss = 0.00434750\n",
      "Iteration 1327, loss = 0.00432950\n",
      "Iteration 1328, loss = 0.00431445\n",
      "Iteration 1329, loss = 0.00433365\n",
      "Iteration 1330, loss = 0.00431654\n",
      "Iteration 1331, loss = 0.00428278\n",
      "Iteration 1332, loss = 0.00429477\n",
      "Iteration 1333, loss = 0.00424448\n",
      "Iteration 1334, loss = 0.00424312\n",
      "Iteration 1335, loss = 0.00424406\n",
      "Iteration 1336, loss = 0.00424281\n",
      "Iteration 1337, loss = 0.00422835\n",
      "Iteration 1338, loss = 0.00421030\n",
      "Iteration 1339, loss = 0.00419375\n",
      "Iteration 1340, loss = 0.00420519\n",
      "Iteration 1341, loss = 0.00416955\n",
      "Iteration 1342, loss = 0.00417832\n",
      "Iteration 1343, loss = 0.00416060\n",
      "Iteration 1344, loss = 0.00414272\n",
      "Iteration 1345, loss = 0.00412400\n",
      "Iteration 1346, loss = 0.00412881\n",
      "Iteration 1347, loss = 0.00412692\n",
      "Iteration 1348, loss = 0.00411801\n",
      "Iteration 1349, loss = 0.00409094\n",
      "Iteration 1350, loss = 0.00408295\n",
      "Iteration 1351, loss = 0.00408429\n",
      "Iteration 1352, loss = 0.00405398\n",
      "Iteration 1353, loss = 0.00405342\n",
      "Iteration 1354, loss = 0.00403899\n",
      "Iteration 1355, loss = 0.00402313\n",
      "Iteration 1356, loss = 0.00403155\n",
      "Iteration 1357, loss = 0.00401262\n",
      "Iteration 1358, loss = 0.00399626\n",
      "Iteration 1359, loss = 0.00398589\n",
      "Iteration 1360, loss = 0.00396809\n",
      "Iteration 1361, loss = 0.00396769\n",
      "Iteration 1362, loss = 0.00397701\n",
      "Iteration 1363, loss = 0.00393777\n",
      "Iteration 1364, loss = 0.00395836\n",
      "Iteration 1365, loss = 0.00392806\n",
      "Iteration 1366, loss = 0.00390936\n",
      "Iteration 1367, loss = 0.00391720\n",
      "Iteration 1368, loss = 0.00391039\n",
      "Iteration 1369, loss = 0.00387728\n",
      "Iteration 1370, loss = 0.00390600\n",
      "Iteration 1371, loss = 0.00387784\n",
      "Iteration 1372, loss = 0.00387768\n",
      "Iteration 1373, loss = 0.00385475\n",
      "Iteration 1374, loss = 0.00385397\n",
      "Iteration 1375, loss = 0.00384537\n",
      "Iteration 1376, loss = 0.00382665\n",
      "Iteration 1377, loss = 0.00381412\n",
      "Iteration 1378, loss = 0.00381439\n",
      "Iteration 1379, loss = 0.00378807\n",
      "Iteration 1380, loss = 0.00378374\n",
      "Iteration 1381, loss = 0.00382141\n",
      "Iteration 1382, loss = 0.00376938\n",
      "Iteration 1383, loss = 0.00376727\n",
      "Iteration 1384, loss = 0.00375675\n",
      "Iteration 1385, loss = 0.00374063\n",
      "Iteration 1386, loss = 0.00376718\n",
      "Iteration 1387, loss = 0.00373108\n",
      "Iteration 1388, loss = 0.00371023\n",
      "Iteration 1389, loss = 0.00369867\n",
      "Iteration 1390, loss = 0.00370134\n",
      "Iteration 1391, loss = 0.00371010\n",
      "Iteration 1392, loss = 0.00367337\n",
      "Iteration 1393, loss = 0.00366528\n",
      "Iteration 1394, loss = 0.00365861\n",
      "Iteration 1395, loss = 0.00364960\n",
      "Iteration 1396, loss = 0.00362807\n",
      "Iteration 1397, loss = 0.00364158\n",
      "Iteration 1398, loss = 0.00361962\n",
      "Iteration 1399, loss = 0.00360894\n",
      "Iteration 1400, loss = 0.00360109\n",
      "Iteration 1401, loss = 0.00361005\n",
      "Iteration 1402, loss = 0.00358473\n",
      "Iteration 1403, loss = 0.00357586\n",
      "Iteration 1404, loss = 0.00356100\n",
      "Iteration 1405, loss = 0.00355723\n",
      "Iteration 1406, loss = 0.00354853\n",
      "Iteration 1407, loss = 0.00354760\n",
      "Iteration 1408, loss = 0.00354677\n",
      "Iteration 1409, loss = 0.00352575\n",
      "Iteration 1410, loss = 0.00352722\n",
      "Iteration 1411, loss = 0.00350620\n",
      "Iteration 1412, loss = 0.00354243\n",
      "Iteration 1413, loss = 0.00352201\n",
      "Iteration 1414, loss = 0.00349647\n",
      "Iteration 1415, loss = 0.00348928\n",
      "Iteration 1416, loss = 0.00346992\n",
      "Iteration 1417, loss = 0.00345833\n",
      "Iteration 1418, loss = 0.00343169\n",
      "Iteration 1419, loss = 0.00344536\n",
      "Iteration 1420, loss = 0.00344657\n",
      "Iteration 1421, loss = 0.00343461\n",
      "Iteration 1422, loss = 0.00341569\n",
      "Iteration 1423, loss = 0.00340790\n",
      "Iteration 1424, loss = 0.00340129\n",
      "Iteration 1425, loss = 0.00339549\n",
      "Iteration 1426, loss = 0.00340431\n",
      "Iteration 1427, loss = 0.00339384\n",
      "Iteration 1428, loss = 0.00339679\n",
      "Iteration 1429, loss = 0.00335666\n",
      "Iteration 1430, loss = 0.00336102\n",
      "Iteration 1431, loss = 0.00337169\n",
      "Iteration 1432, loss = 0.00334489\n",
      "Iteration 1433, loss = 0.00334286\n",
      "Iteration 1434, loss = 0.00334633\n",
      "Iteration 1435, loss = 0.00332016\n",
      "Iteration 1436, loss = 0.00332322\n",
      "Iteration 1437, loss = 0.00329897\n",
      "Iteration 1438, loss = 0.00330300\n",
      "Iteration 1439, loss = 0.00328365\n",
      "Iteration 1440, loss = 0.00326984\n",
      "Iteration 1441, loss = 0.00327428\n",
      "Iteration 1442, loss = 0.00326812\n",
      "Iteration 1443, loss = 0.00325525\n",
      "Iteration 1444, loss = 0.00326718\n",
      "Iteration 1445, loss = 0.00323361\n",
      "Iteration 1446, loss = 0.00322991\n",
      "Iteration 1447, loss = 0.00323577\n",
      "Iteration 1448, loss = 0.00321791\n",
      "Iteration 1449, loss = 0.00319013\n",
      "Iteration 1450, loss = 0.00325896\n",
      "Iteration 1451, loss = 0.00322261\n",
      "Iteration 1452, loss = 0.00319195\n",
      "Iteration 1453, loss = 0.00317982\n",
      "Iteration 1454, loss = 0.00319747\n",
      "Iteration 1455, loss = 0.00317591\n",
      "Iteration 1456, loss = 0.00314327\n",
      "Iteration 1457, loss = 0.00315122\n",
      "Iteration 1458, loss = 0.00314143\n",
      "Iteration 1459, loss = 0.00315122\n",
      "Iteration 1460, loss = 0.00312950\n",
      "Iteration 1461, loss = 0.00310798\n",
      "Iteration 1462, loss = 0.00312021\n",
      "Iteration 1463, loss = 0.00313201\n",
      "Iteration 1464, loss = 0.00309312\n",
      "Iteration 1465, loss = 0.00308995\n",
      "Iteration 1466, loss = 0.00310487\n",
      "Iteration 1467, loss = 0.00308985\n",
      "Iteration 1468, loss = 0.00306769\n",
      "Iteration 1469, loss = 0.00305546\n",
      "Iteration 1470, loss = 0.00307273\n",
      "Iteration 1471, loss = 0.00305209\n",
      "Iteration 1472, loss = 0.00305320\n",
      "Iteration 1473, loss = 0.00305143\n",
      "Iteration 1474, loss = 0.00302231\n",
      "Iteration 1475, loss = 0.00302493\n",
      "Iteration 1476, loss = 0.00301282\n",
      "Iteration 1477, loss = 0.00297410\n",
      "Iteration 1478, loss = 0.00299653\n",
      "Iteration 1479, loss = 0.00301837\n",
      "Iteration 1480, loss = 0.00301695\n",
      "Iteration 1481, loss = 0.00301122\n",
      "Iteration 1482, loss = 0.00297880\n",
      "Iteration 1483, loss = 0.00297611\n",
      "Iteration 1484, loss = 0.00294717\n",
      "Iteration 1485, loss = 0.00296776\n",
      "Iteration 1486, loss = 0.00296937\n",
      "Iteration 1487, loss = 0.00293251\n",
      "Iteration 1488, loss = 0.00292485\n",
      "Iteration 1489, loss = 0.00296663\n",
      "Iteration 1490, loss = 0.00292204\n",
      "Iteration 1491, loss = 0.00290036\n",
      "Iteration 1492, loss = 0.00290580\n",
      "Iteration 1493, loss = 0.00290855\n",
      "Iteration 1494, loss = 0.00288653\n",
      "Iteration 1495, loss = 0.00290551\n",
      "Iteration 1496, loss = 0.00289555\n",
      "Iteration 1497, loss = 0.00290594\n",
      "Iteration 1498, loss = 0.00287433\n",
      "Iteration 1499, loss = 0.00285474\n",
      "Iteration 1500, loss = 0.00286183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ville\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-8 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-8 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-8 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-8 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-8 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-8 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-8 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-8 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-8 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-8 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(2, 2), max_iter=1500, tol=1e-05, verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_credit = MLPClassifier(max_iter = 1500, \n",
    "                              verbose = True, # True show the loss for each iteration\n",
    "                              tol = 0.0000100,\n",
    "                              solver = 'adam',\n",
    "                              activation = 'relu',\n",
    "                              hidden_layer_sizes = (2,2)# 3 neurons input -> 2 hidden -> 2 hidden -> 1 neuron output\n",
    "                                ) \n",
    "neural_credit.fit(x_credit_training,y_credit_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = neural_credit.predict(x_credit_test)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "accuracy_score(y_credit_test,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.50074880\n",
      "Iteration 2, loss = 0.41569587\n",
      "Iteration 3, loss = 0.35125549\n",
      "Iteration 4, loss = 0.29936157\n",
      "Iteration 5, loss = 0.25686172\n",
      "Iteration 6, loss = 0.22183389\n",
      "Iteration 7, loss = 0.19373108\n",
      "Iteration 8, loss = 0.17110787\n",
      "Iteration 9, loss = 0.15379089\n",
      "Iteration 10, loss = 0.13953422\n",
      "Iteration 11, loss = 0.12799467\n",
      "Iteration 12, loss = 0.11853303\n",
      "Iteration 13, loss = 0.11033069\n",
      "Iteration 14, loss = 0.10344886\n",
      "Iteration 15, loss = 0.09714547\n",
      "Iteration 16, loss = 0.09126802\n",
      "Iteration 17, loss = 0.08613485\n",
      "Iteration 18, loss = 0.08167044\n",
      "Iteration 19, loss = 0.07797665\n",
      "Iteration 20, loss = 0.07318843\n",
      "Iteration 21, loss = 0.07002657\n",
      "Iteration 22, loss = 0.06655035\n",
      "Iteration 23, loss = 0.06368770\n",
      "Iteration 24, loss = 0.06117564\n",
      "Iteration 25, loss = 0.05848723\n",
      "Iteration 26, loss = 0.05604054\n",
      "Iteration 27, loss = 0.05378739\n",
      "Iteration 28, loss = 0.05193969\n",
      "Iteration 29, loss = 0.04995472\n",
      "Iteration 30, loss = 0.04809029\n",
      "Iteration 31, loss = 0.04638432\n",
      "Iteration 32, loss = 0.04481587\n",
      "Iteration 33, loss = 0.04349650\n",
      "Iteration 34, loss = 0.04199751\n",
      "Iteration 35, loss = 0.04064808\n",
      "Iteration 36, loss = 0.03952569\n",
      "Iteration 37, loss = 0.03819181\n",
      "Iteration 38, loss = 0.03710454\n",
      "Iteration 39, loss = 0.03600621\n",
      "Iteration 40, loss = 0.03512774\n",
      "Iteration 41, loss = 0.03421497\n",
      "Iteration 42, loss = 0.03327498\n",
      "Iteration 43, loss = 0.03276985\n",
      "Iteration 44, loss = 0.03175195\n",
      "Iteration 45, loss = 0.03105536\n",
      "Iteration 46, loss = 0.03039712\n",
      "Iteration 47, loss = 0.02961589\n",
      "Iteration 48, loss = 0.02880304\n",
      "Iteration 49, loss = 0.02804382\n",
      "Iteration 50, loss = 0.02751051\n",
      "Iteration 51, loss = 0.02719298\n",
      "Iteration 52, loss = 0.02635848\n",
      "Iteration 53, loss = 0.02613045\n",
      "Iteration 54, loss = 0.02518204\n",
      "Iteration 55, loss = 0.02467756\n",
      "Iteration 56, loss = 0.02420007\n",
      "Iteration 57, loss = 0.02380768\n",
      "Iteration 58, loss = 0.02324184\n",
      "Iteration 59, loss = 0.02275010\n",
      "Iteration 60, loss = 0.02239682\n",
      "Iteration 61, loss = 0.02207611\n",
      "Iteration 62, loss = 0.02175865\n",
      "Iteration 63, loss = 0.02136972\n",
      "Iteration 64, loss = 0.02087061\n",
      "Iteration 65, loss = 0.02062853\n",
      "Iteration 66, loss = 0.02042863\n",
      "Iteration 67, loss = 0.02011291\n",
      "Iteration 68, loss = 0.01968950\n",
      "Iteration 69, loss = 0.01927751\n",
      "Iteration 70, loss = 0.01909992\n",
      "Iteration 71, loss = 0.01890296\n",
      "Iteration 72, loss = 0.01897892\n",
      "Iteration 73, loss = 0.01837619\n",
      "Iteration 74, loss = 0.01795193\n",
      "Iteration 75, loss = 0.01786439\n",
      "Iteration 76, loss = 0.01743056\n",
      "Iteration 77, loss = 0.01718756\n",
      "Iteration 78, loss = 0.01698452\n",
      "Iteration 79, loss = 0.01692840\n",
      "Iteration 80, loss = 0.01651056\n",
      "Iteration 81, loss = 0.01628995\n",
      "Iteration 82, loss = 0.01619462\n",
      "Iteration 83, loss = 0.01592362\n",
      "Iteration 84, loss = 0.01560666\n",
      "Iteration 85, loss = 0.01540522\n",
      "Iteration 86, loss = 0.01553713\n",
      "Iteration 87, loss = 0.01513767\n",
      "Iteration 88, loss = 0.01496643\n",
      "Iteration 89, loss = 0.01518833\n",
      "Iteration 90, loss = 0.01543749\n",
      "Iteration 91, loss = 0.01460211\n",
      "Iteration 92, loss = 0.01431619\n",
      "Iteration 93, loss = 0.01400761\n",
      "Iteration 94, loss = 0.01389176\n",
      "Iteration 95, loss = 0.01371676\n",
      "Iteration 96, loss = 0.01372687\n",
      "Iteration 97, loss = 0.01338779\n",
      "Iteration 98, loss = 0.01326811\n",
      "Iteration 99, loss = 0.01327024\n",
      "Iteration 100, loss = 0.01324652\n",
      "Iteration 101, loss = 0.01299760\n",
      "Iteration 102, loss = 0.01329141\n",
      "Iteration 103, loss = 0.01286029\n",
      "Iteration 104, loss = 0.01242098\n",
      "Iteration 105, loss = 0.01283966\n",
      "Iteration 106, loss = 0.01213266\n",
      "Iteration 107, loss = 0.01224880\n",
      "Iteration 108, loss = 0.01190956\n",
      "Iteration 109, loss = 0.01205944\n",
      "Iteration 110, loss = 0.01168575\n",
      "Iteration 111, loss = 0.01206735\n",
      "Iteration 112, loss = 0.01200310\n",
      "Iteration 113, loss = 0.01166917\n",
      "Iteration 114, loss = 0.01142856\n",
      "Iteration 115, loss = 0.01121104\n",
      "Iteration 116, loss = 0.01107639\n",
      "Iteration 117, loss = 0.01108494\n",
      "Iteration 118, loss = 0.01081452\n",
      "Iteration 119, loss = 0.01096717\n",
      "Iteration 120, loss = 0.01059616\n",
      "Iteration 121, loss = 0.01051107\n",
      "Iteration 122, loss = 0.01073672\n",
      "Iteration 123, loss = 0.01066464\n",
      "Iteration 124, loss = 0.01018845\n",
      "Iteration 125, loss = 0.01014876\n",
      "Iteration 126, loss = 0.01008695\n",
      "Iteration 127, loss = 0.01019155\n",
      "Iteration 128, loss = 0.01012118\n",
      "Iteration 129, loss = 0.00972373\n",
      "Iteration 130, loss = 0.00986043\n",
      "Iteration 131, loss = 0.00958432\n",
      "Iteration 132, loss = 0.00951862\n",
      "Iteration 133, loss = 0.00945304\n",
      "Iteration 134, loss = 0.00946961\n",
      "Iteration 135, loss = 0.00941953\n",
      "Iteration 136, loss = 0.00925321\n",
      "Iteration 137, loss = 0.00900550\n",
      "Iteration 138, loss = 0.00907875\n",
      "Iteration 139, loss = 0.00909812\n",
      "Iteration 140, loss = 0.00877200\n",
      "Iteration 141, loss = 0.00876671\n",
      "Iteration 142, loss = 0.00875508\n",
      "Iteration 143, loss = 0.00868171\n",
      "Iteration 144, loss = 0.00890032\n",
      "Iteration 145, loss = 0.00868413\n",
      "Iteration 146, loss = 0.00838002\n",
      "Iteration 147, loss = 0.00847497\n",
      "Iteration 148, loss = 0.00870174\n",
      "Iteration 149, loss = 0.00854896\n",
      "Iteration 150, loss = 0.00818537\n",
      "Iteration 151, loss = 0.00818562\n",
      "Iteration 152, loss = 0.00815400\n",
      "Iteration 153, loss = 0.00824032\n",
      "Iteration 154, loss = 0.00870866\n",
      "Iteration 155, loss = 0.00830979\n",
      "Iteration 156, loss = 0.00766338\n",
      "Iteration 157, loss = 0.00782827\n",
      "Iteration 158, loss = 0.00764290\n",
      "Iteration 159, loss = 0.00759706\n",
      "Iteration 160, loss = 0.00751848\n",
      "Iteration 161, loss = 0.00751276\n",
      "Iteration 162, loss = 0.00745824\n",
      "Iteration 163, loss = 0.00727945\n",
      "Iteration 164, loss = 0.00748395\n",
      "Iteration 165, loss = 0.00732126\n",
      "Iteration 166, loss = 0.00739803\n",
      "Iteration 167, loss = 0.00726591\n",
      "Iteration 168, loss = 0.00731227\n",
      "Iteration 169, loss = 0.00695684\n",
      "Iteration 170, loss = 0.00700363\n",
      "Iteration 171, loss = 0.00713519\n",
      "Iteration 172, loss = 0.00706392\n",
      "Iteration 173, loss = 0.00729028\n",
      "Iteration 174, loss = 0.00854442\n",
      "Iteration 175, loss = 0.00683409\n",
      "Iteration 176, loss = 0.00676259\n",
      "Iteration 177, loss = 0.00691370\n",
      "Iteration 178, loss = 0.00662151\n",
      "Iteration 179, loss = 0.00662991\n",
      "Iteration 180, loss = 0.00675679\n",
      "Iteration 181, loss = 0.00647666\n",
      "Iteration 182, loss = 0.00649031\n",
      "Iteration 183, loss = 0.00631514\n",
      "Iteration 184, loss = 0.00624407\n",
      "Iteration 185, loss = 0.00625394\n",
      "Iteration 186, loss = 0.00635692\n",
      "Iteration 187, loss = 0.00638544\n",
      "Iteration 188, loss = 0.00607284\n",
      "Iteration 189, loss = 0.00613040\n",
      "Iteration 190, loss = 0.00617276\n",
      "Iteration 191, loss = 0.00601952\n",
      "Iteration 192, loss = 0.00603938\n",
      "Iteration 193, loss = 0.00587097\n",
      "Iteration 194, loss = 0.00606578\n",
      "Iteration 195, loss = 0.00605524\n",
      "Iteration 196, loss = 0.00571281\n",
      "Iteration 197, loss = 0.00589567\n",
      "Iteration 198, loss = 0.00585578\n",
      "Iteration 199, loss = 0.00575029\n",
      "Iteration 200, loss = 0.00563108\n",
      "Iteration 201, loss = 0.00567438\n",
      "Iteration 202, loss = 0.00557438\n",
      "Iteration 203, loss = 0.00557203\n",
      "Iteration 204, loss = 0.00559320\n",
      "Iteration 205, loss = 0.00559151\n",
      "Iteration 206, loss = 0.00539984\n",
      "Iteration 207, loss = 0.00535134\n",
      "Iteration 208, loss = 0.00572766\n",
      "Iteration 209, loss = 0.00548633\n",
      "Iteration 210, loss = 0.00551621\n",
      "Iteration 211, loss = 0.00543522\n",
      "Iteration 212, loss = 0.00526172\n",
      "Iteration 213, loss = 0.00515632\n",
      "Iteration 214, loss = 0.00516161\n",
      "Iteration 215, loss = 0.00528889\n",
      "Iteration 216, loss = 0.00523046\n",
      "Iteration 217, loss = 0.00526638\n",
      "Iteration 218, loss = 0.00515277\n",
      "Iteration 219, loss = 0.00525171\n",
      "Iteration 220, loss = 0.00515872\n",
      "Iteration 221, loss = 0.00491864\n",
      "Iteration 222, loss = 0.00492478\n",
      "Iteration 223, loss = 0.00488503\n",
      "Iteration 224, loss = 0.00498004\n",
      "Iteration 225, loss = 0.00480684\n",
      "Iteration 226, loss = 0.00489895\n",
      "Iteration 227, loss = 0.00494548\n",
      "Iteration 228, loss = 0.00510557\n",
      "Iteration 229, loss = 0.00557945\n",
      "Iteration 230, loss = 0.00484823\n",
      "Iteration 231, loss = 0.00499084\n",
      "Iteration 232, loss = 0.00466418\n",
      "Iteration 233, loss = 0.00469324\n",
      "Iteration 234, loss = 0.00474010\n",
      "Iteration 235, loss = 0.00482081\n",
      "Iteration 236, loss = 0.00459663\n",
      "Iteration 237, loss = 0.00459431\n",
      "Iteration 238, loss = 0.00459011\n",
      "Iteration 239, loss = 0.00455665\n",
      "Iteration 240, loss = 0.00443826\n",
      "Iteration 241, loss = 0.00438270\n",
      "Iteration 242, loss = 0.00439491\n",
      "Iteration 243, loss = 0.00431465\n",
      "Iteration 244, loss = 0.00438271\n",
      "Iteration 245, loss = 0.00422561\n",
      "Iteration 246, loss = 0.00445566\n",
      "Iteration 247, loss = 0.00443166\n",
      "Iteration 248, loss = 0.00418260\n",
      "Iteration 249, loss = 0.00438980\n",
      "Iteration 250, loss = 0.00416012\n",
      "Iteration 251, loss = 0.00408507\n",
      "Iteration 252, loss = 0.00418863\n",
      "Iteration 253, loss = 0.00415962\n",
      "Iteration 254, loss = 0.00410808\n",
      "Iteration 255, loss = 0.00415807\n",
      "Iteration 256, loss = 0.00397972\n",
      "Iteration 257, loss = 0.00399810\n",
      "Iteration 258, loss = 0.00421743\n",
      "Iteration 259, loss = 0.00389273\n",
      "Iteration 260, loss = 0.00404768\n",
      "Iteration 261, loss = 0.00387493\n",
      "Iteration 262, loss = 0.00386095\n",
      "Iteration 263, loss = 0.00382674\n",
      "Iteration 264, loss = 0.00385778\n",
      "Iteration 265, loss = 0.00379269\n",
      "Iteration 266, loss = 0.00375374\n",
      "Iteration 267, loss = 0.00402063\n",
      "Iteration 268, loss = 0.00370245\n",
      "Iteration 269, loss = 0.00372429\n",
      "Iteration 270, loss = 0.00364574\n",
      "Iteration 271, loss = 0.00385480\n",
      "Iteration 272, loss = 0.00376355\n",
      "Iteration 273, loss = 0.00371922\n",
      "Iteration 274, loss = 0.00378746\n",
      "Iteration 275, loss = 0.00353174\n",
      "Iteration 276, loss = 0.00367376\n",
      "Iteration 277, loss = 0.00351108\n",
      "Iteration 278, loss = 0.00350995\n",
      "Iteration 279, loss = 0.00353919\n",
      "Iteration 280, loss = 0.00379103\n",
      "Iteration 281, loss = 0.00347682\n",
      "Iteration 282, loss = 0.00356030\n",
      "Iteration 283, loss = 0.00336912\n",
      "Iteration 284, loss = 0.00345347\n",
      "Iteration 285, loss = 0.00336267\n",
      "Iteration 286, loss = 0.00338771\n",
      "Iteration 287, loss = 0.00349067\n",
      "Iteration 288, loss = 0.00335394\n",
      "Iteration 289, loss = 0.00353640\n",
      "Iteration 290, loss = 0.00392293\n",
      "Iteration 291, loss = 0.00387798\n",
      "Iteration 292, loss = 0.00336572\n",
      "Iteration 293, loss = 0.00363893\n",
      "Iteration 294, loss = 0.00354878\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-9 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-9 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-9 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-9 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-9 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-9 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-9 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-9 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-9 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-9 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50, 50), max_iter=1500, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_credit = MLPClassifier(max_iter = 1500, \n",
    "                              verbose = True, # True show the loss for each iteration\n",
    "                              tol = 0.0000100,\n",
    "                              solver = 'adam',\n",
    "                              activation = 'relu',\n",
    "                              hidden_layer_sizes = (50,50)# 3 neurons input -> 2 hidden -> 2 hidden -> 1 neuron output\n",
    "                                ) \n",
    "neural_credit.fit(x_credit_training,y_credit_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = neural_credit.predict(x_credit_test)\n",
    "accuracy_score(y_credit_test,prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More complex structure doesn't mean better results/accuracy (ex: 50 neurons vs 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.55120474\n",
      "Iteration 2, loss = 0.54138948\n",
      "Iteration 3, loss = 0.53210430\n",
      "Iteration 4, loss = 0.52350898\n",
      "Iteration 5, loss = 0.51559908\n",
      "Iteration 6, loss = 0.50817940\n",
      "Iteration 7, loss = 0.50137905\n",
      "Iteration 8, loss = 0.49495268\n",
      "Iteration 9, loss = 0.48896795\n",
      "Iteration 10, loss = 0.48320766\n",
      "Iteration 11, loss = 0.47780719\n",
      "Iteration 12, loss = 0.47273509\n",
      "Iteration 13, loss = 0.46780920\n",
      "Iteration 14, loss = 0.46307750\n",
      "Iteration 15, loss = 0.45846730\n",
      "Iteration 16, loss = 0.45397672\n",
      "Iteration 17, loss = 0.44971005\n",
      "Iteration 18, loss = 0.44555826\n",
      "Iteration 19, loss = 0.44137715\n",
      "Iteration 20, loss = 0.43752073\n",
      "Iteration 21, loss = 0.43356424\n",
      "Iteration 22, loss = 0.42984965\n",
      "Iteration 23, loss = 0.42615257\n",
      "Iteration 24, loss = 0.42263378\n",
      "Iteration 25, loss = 0.41924049\n",
      "Iteration 26, loss = 0.41593748\n",
      "Iteration 27, loss = 0.41268062\n",
      "Iteration 28, loss = 0.40974005\n",
      "Iteration 29, loss = 0.40678189\n",
      "Iteration 30, loss = 0.40404245\n",
      "Iteration 31, loss = 0.40140295\n",
      "Iteration 32, loss = 0.39886437\n",
      "Iteration 33, loss = 0.39633301\n",
      "Iteration 34, loss = 0.39398073\n",
      "Iteration 35, loss = 0.39166463\n",
      "Iteration 36, loss = 0.38946212\n",
      "Iteration 37, loss = 0.38736523\n",
      "Iteration 38, loss = 0.38526868\n",
      "Iteration 39, loss = 0.38330559\n",
      "Iteration 40, loss = 0.38135621\n",
      "Iteration 41, loss = 0.37943054\n",
      "Iteration 42, loss = 0.37759776\n",
      "Iteration 43, loss = 0.37572596\n",
      "Iteration 44, loss = 0.37387622\n",
      "Iteration 45, loss = 0.37210656\n",
      "Iteration 46, loss = 0.37032735\n",
      "Iteration 47, loss = 0.36854541\n",
      "Iteration 48, loss = 0.36679712\n",
      "Iteration 49, loss = 0.36506030\n",
      "Iteration 50, loss = 0.36329974\n",
      "Iteration 51, loss = 0.36154580\n",
      "Iteration 52, loss = 0.35979716\n",
      "Iteration 53, loss = 0.35802134\n",
      "Iteration 54, loss = 0.35628037\n",
      "Iteration 55, loss = 0.35449880\n",
      "Iteration 56, loss = 0.35274888\n",
      "Iteration 57, loss = 0.35094243\n",
      "Iteration 58, loss = 0.34917470\n",
      "Iteration 59, loss = 0.34739534\n",
      "Iteration 60, loss = 0.34561838\n",
      "Iteration 61, loss = 0.34380795\n",
      "Iteration 62, loss = 0.34199309\n",
      "Iteration 63, loss = 0.34013809\n",
      "Iteration 64, loss = 0.33833683\n",
      "Iteration 65, loss = 0.33646288\n",
      "Iteration 66, loss = 0.33463480\n",
      "Iteration 67, loss = 0.33279477\n",
      "Iteration 68, loss = 0.33095223\n",
      "Iteration 69, loss = 0.32915218\n",
      "Iteration 70, loss = 0.32729036\n",
      "Iteration 71, loss = 0.32543208\n",
      "Iteration 72, loss = 0.32360544\n",
      "Iteration 73, loss = 0.32171435\n",
      "Iteration 74, loss = 0.31979394\n",
      "Iteration 75, loss = 0.31790707\n",
      "Iteration 76, loss = 0.31600382\n",
      "Iteration 77, loss = 0.31404035\n",
      "Iteration 78, loss = 0.31205091\n",
      "Iteration 79, loss = 0.31000569\n",
      "Iteration 80, loss = 0.30791632\n",
      "Iteration 81, loss = 0.30590770\n",
      "Iteration 82, loss = 0.30377184\n",
      "Iteration 83, loss = 0.30164826\n",
      "Iteration 84, loss = 0.29957391\n",
      "Iteration 85, loss = 0.29732744\n",
      "Iteration 86, loss = 0.29515656\n",
      "Iteration 87, loss = 0.29295387\n",
      "Iteration 88, loss = 0.29067886\n",
      "Iteration 89, loss = 0.28842863\n",
      "Iteration 90, loss = 0.28615390\n",
      "Iteration 91, loss = 0.28384210\n",
      "Iteration 92, loss = 0.28146989\n",
      "Iteration 93, loss = 0.27923517\n",
      "Iteration 94, loss = 0.27687825\n",
      "Iteration 95, loss = 0.27460157\n",
      "Iteration 96, loss = 0.27235149\n",
      "Iteration 97, loss = 0.27005289\n",
      "Iteration 98, loss = 0.26778145\n",
      "Iteration 99, loss = 0.26550023\n",
      "Iteration 100, loss = 0.26331959\n",
      "Iteration 101, loss = 0.26103849\n",
      "Iteration 102, loss = 0.25885446\n",
      "Iteration 103, loss = 0.25668532\n",
      "Iteration 104, loss = 0.25452817\n",
      "Iteration 105, loss = 0.25229922\n",
      "Iteration 106, loss = 0.25006294\n",
      "Iteration 107, loss = 0.24792159\n",
      "Iteration 108, loss = 0.24577416\n",
      "Iteration 109, loss = 0.24361456\n",
      "Iteration 110, loss = 0.24148456\n",
      "Iteration 111, loss = 0.23934177\n",
      "Iteration 112, loss = 0.23725604\n",
      "Iteration 113, loss = 0.23520566\n",
      "Iteration 114, loss = 0.23312179\n",
      "Iteration 115, loss = 0.23109291\n",
      "Iteration 116, loss = 0.22906989\n",
      "Iteration 117, loss = 0.22714683\n",
      "Iteration 118, loss = 0.22513446\n",
      "Iteration 119, loss = 0.22332443\n",
      "Iteration 120, loss = 0.22126361\n",
      "Iteration 121, loss = 0.21935087\n",
      "Iteration 122, loss = 0.21739706\n",
      "Iteration 123, loss = 0.21550078\n",
      "Iteration 124, loss = 0.21371142\n",
      "Iteration 125, loss = 0.21182224\n",
      "Iteration 126, loss = 0.21000455\n",
      "Iteration 127, loss = 0.20819474\n",
      "Iteration 128, loss = 0.20637864\n",
      "Iteration 129, loss = 0.20457180\n",
      "Iteration 130, loss = 0.20284149\n",
      "Iteration 131, loss = 0.20116957\n",
      "Iteration 132, loss = 0.19942835\n",
      "Iteration 133, loss = 0.19774970\n",
      "Iteration 134, loss = 0.19611428\n",
      "Iteration 135, loss = 0.19456250\n",
      "Iteration 136, loss = 0.19290914\n",
      "Iteration 137, loss = 0.19129743\n",
      "Iteration 138, loss = 0.18974541\n",
      "Iteration 139, loss = 0.18820362\n",
      "Iteration 140, loss = 0.18672046\n",
      "Iteration 141, loss = 0.18514107\n",
      "Iteration 142, loss = 0.18366859\n",
      "Iteration 143, loss = 0.18212568\n",
      "Iteration 144, loss = 0.18072475\n",
      "Iteration 145, loss = 0.17924566\n",
      "Iteration 146, loss = 0.17783851\n",
      "Iteration 147, loss = 0.17644056\n",
      "Iteration 148, loss = 0.17516610\n",
      "Iteration 149, loss = 0.17376500\n",
      "Iteration 150, loss = 0.17244958\n",
      "Iteration 151, loss = 0.17115335\n",
      "Iteration 152, loss = 0.16989618\n",
      "Iteration 153, loss = 0.16857420\n",
      "Iteration 154, loss = 0.16734054\n",
      "Iteration 155, loss = 0.16613424\n",
      "Iteration 156, loss = 0.16489627\n",
      "Iteration 157, loss = 0.16374864\n",
      "Iteration 158, loss = 0.16250484\n",
      "Iteration 159, loss = 0.16142276\n",
      "Iteration 160, loss = 0.16034091\n",
      "Iteration 161, loss = 0.15918259\n",
      "Iteration 162, loss = 0.15805450\n",
      "Iteration 163, loss = 0.15701597\n",
      "Iteration 164, loss = 0.15597018\n",
      "Iteration 165, loss = 0.15497333\n",
      "Iteration 166, loss = 0.15395954\n",
      "Iteration 167, loss = 0.15298021\n",
      "Iteration 168, loss = 0.15201274\n",
      "Iteration 169, loss = 0.15095517\n",
      "Iteration 170, loss = 0.15004879\n",
      "Iteration 171, loss = 0.14915983\n",
      "Iteration 172, loss = 0.14832525\n",
      "Iteration 173, loss = 0.14748074\n",
      "Iteration 174, loss = 0.14671049\n",
      "Iteration 175, loss = 0.14591121\n",
      "Iteration 176, loss = 0.14513818\n",
      "Iteration 177, loss = 0.14436790\n",
      "Iteration 178, loss = 0.14359417\n",
      "Iteration 179, loss = 0.14295435\n",
      "Iteration 180, loss = 0.14217549\n",
      "Iteration 181, loss = 0.14152565\n",
      "Iteration 182, loss = 0.14087779\n",
      "Iteration 183, loss = 0.14015033\n",
      "Iteration 184, loss = 0.13946937\n",
      "Iteration 185, loss = 0.13881078\n",
      "Iteration 186, loss = 0.13816879\n",
      "Iteration 187, loss = 0.13754353\n",
      "Iteration 188, loss = 0.13693877\n",
      "Iteration 189, loss = 0.13622137\n",
      "Iteration 190, loss = 0.13560150\n",
      "Iteration 191, loss = 0.13492269\n",
      "Iteration 192, loss = 0.13433649\n",
      "Iteration 193, loss = 0.13366243\n",
      "Iteration 194, loss = 0.13303364\n",
      "Iteration 195, loss = 0.13237871\n",
      "Iteration 196, loss = 0.13170113\n",
      "Iteration 197, loss = 0.13113132\n",
      "Iteration 198, loss = 0.13044731\n",
      "Iteration 199, loss = 0.12982569\n",
      "Iteration 200, loss = 0.12937885\n",
      "Iteration 201, loss = 0.12866127\n",
      "Iteration 202, loss = 0.12807407\n",
      "Iteration 203, loss = 0.12749278\n",
      "Iteration 204, loss = 0.12678384\n",
      "Iteration 205, loss = 0.12610912\n",
      "Iteration 206, loss = 0.12547615\n",
      "Iteration 207, loss = 0.12471860\n",
      "Iteration 208, loss = 0.12402179\n",
      "Iteration 209, loss = 0.12334749\n",
      "Iteration 210, loss = 0.12254483\n",
      "Iteration 211, loss = 0.12181642\n",
      "Iteration 212, loss = 0.12106668\n",
      "Iteration 213, loss = 0.12035703\n",
      "Iteration 214, loss = 0.11971403\n",
      "Iteration 215, loss = 0.11899885\n",
      "Iteration 216, loss = 0.11833182\n",
      "Iteration 217, loss = 0.11773223\n",
      "Iteration 218, loss = 0.11698883\n",
      "Iteration 219, loss = 0.11633689\n",
      "Iteration 220, loss = 0.11564846\n",
      "Iteration 221, loss = 0.11495979\n",
      "Iteration 222, loss = 0.11431373\n",
      "Iteration 223, loss = 0.11369555\n",
      "Iteration 224, loss = 0.11298034\n",
      "Iteration 225, loss = 0.11231856\n",
      "Iteration 226, loss = 0.11161280\n",
      "Iteration 227, loss = 0.11091940\n",
      "Iteration 228, loss = 0.11026370\n",
      "Iteration 229, loss = 0.10960261\n",
      "Iteration 230, loss = 0.10896891\n",
      "Iteration 231, loss = 0.10834056\n",
      "Iteration 232, loss = 0.10768102\n",
      "Iteration 233, loss = 0.10702722\n",
      "Iteration 234, loss = 0.10641258\n",
      "Iteration 235, loss = 0.10578239\n",
      "Iteration 236, loss = 0.10509326\n",
      "Iteration 237, loss = 0.10447435\n",
      "Iteration 238, loss = 0.10378705\n",
      "Iteration 239, loss = 0.10315507\n",
      "Iteration 240, loss = 0.10256637\n",
      "Iteration 241, loss = 0.10186625\n",
      "Iteration 242, loss = 0.10122827\n",
      "Iteration 243, loss = 0.10061724\n",
      "Iteration 244, loss = 0.09995998\n",
      "Iteration 245, loss = 0.09927529\n",
      "Iteration 246, loss = 0.09865320\n",
      "Iteration 247, loss = 0.09798882\n",
      "Iteration 248, loss = 0.09734158\n",
      "Iteration 249, loss = 0.09668223\n",
      "Iteration 250, loss = 0.09601325\n",
      "Iteration 251, loss = 0.09540448\n",
      "Iteration 252, loss = 0.09474669\n",
      "Iteration 253, loss = 0.09408313\n",
      "Iteration 254, loss = 0.09346523\n",
      "Iteration 255, loss = 0.09284144\n",
      "Iteration 256, loss = 0.09223389\n",
      "Iteration 257, loss = 0.09156628\n",
      "Iteration 258, loss = 0.09096810\n",
      "Iteration 259, loss = 0.09026821\n",
      "Iteration 260, loss = 0.08966654\n",
      "Iteration 261, loss = 0.08906142\n",
      "Iteration 262, loss = 0.08841017\n",
      "Iteration 263, loss = 0.08781235\n",
      "Iteration 264, loss = 0.08726146\n",
      "Iteration 265, loss = 0.08660819\n",
      "Iteration 266, loss = 0.08600049\n",
      "Iteration 267, loss = 0.08544187\n",
      "Iteration 268, loss = 0.08479436\n",
      "Iteration 269, loss = 0.08418975\n",
      "Iteration 270, loss = 0.08356959\n",
      "Iteration 271, loss = 0.08296631\n",
      "Iteration 272, loss = 0.08238094\n",
      "Iteration 273, loss = 0.08177760\n",
      "Iteration 274, loss = 0.08122942\n",
      "Iteration 275, loss = 0.08061075\n",
      "Iteration 276, loss = 0.08002031\n",
      "Iteration 277, loss = 0.07944198\n",
      "Iteration 278, loss = 0.07886403\n",
      "Iteration 279, loss = 0.07830743\n",
      "Iteration 280, loss = 0.07776518\n",
      "Iteration 281, loss = 0.07722461\n",
      "Iteration 282, loss = 0.07666662\n",
      "Iteration 283, loss = 0.07609613\n",
      "Iteration 284, loss = 0.07555988\n",
      "Iteration 285, loss = 0.07497028\n",
      "Iteration 286, loss = 0.07444403\n",
      "Iteration 287, loss = 0.07386289\n",
      "Iteration 288, loss = 0.07333116\n",
      "Iteration 289, loss = 0.07278337\n",
      "Iteration 290, loss = 0.07226863\n",
      "Iteration 291, loss = 0.07174266\n",
      "Iteration 292, loss = 0.07121410\n",
      "Iteration 293, loss = 0.07069944\n",
      "Iteration 294, loss = 0.07021049\n",
      "Iteration 295, loss = 0.06967089\n",
      "Iteration 296, loss = 0.06920546\n",
      "Iteration 297, loss = 0.06875371\n",
      "Iteration 298, loss = 0.06826754\n",
      "Iteration 299, loss = 0.06779299\n",
      "Iteration 300, loss = 0.06733531\n",
      "Iteration 301, loss = 0.06689441\n",
      "Iteration 302, loss = 0.06644727\n",
      "Iteration 303, loss = 0.06600279\n",
      "Iteration 304, loss = 0.06555954\n",
      "Iteration 305, loss = 0.06515069\n",
      "Iteration 306, loss = 0.06475711\n",
      "Iteration 307, loss = 0.06434513\n",
      "Iteration 308, loss = 0.06384420\n",
      "Iteration 309, loss = 0.06345033\n",
      "Iteration 310, loss = 0.06304276\n",
      "Iteration 311, loss = 0.06266064\n",
      "Iteration 312, loss = 0.06220371\n",
      "Iteration 313, loss = 0.06179781\n",
      "Iteration 314, loss = 0.06144251\n",
      "Iteration 315, loss = 0.06102880\n",
      "Iteration 316, loss = 0.06067302\n",
      "Iteration 317, loss = 0.06028192\n",
      "Iteration 318, loss = 0.05988292\n",
      "Iteration 319, loss = 0.05951533\n",
      "Iteration 320, loss = 0.05915589\n",
      "Iteration 321, loss = 0.05875551\n",
      "Iteration 322, loss = 0.05839485\n",
      "Iteration 323, loss = 0.05800103\n",
      "Iteration 324, loss = 0.05765812\n",
      "Iteration 325, loss = 0.05731005\n",
      "Iteration 326, loss = 0.05694902\n",
      "Iteration 327, loss = 0.05666664\n",
      "Iteration 328, loss = 0.05628583\n",
      "Iteration 329, loss = 0.05590898\n",
      "Iteration 330, loss = 0.05555858\n",
      "Iteration 331, loss = 0.05525608\n",
      "Iteration 332, loss = 0.05489074\n",
      "Iteration 333, loss = 0.05460090\n",
      "Iteration 334, loss = 0.05418677\n",
      "Iteration 335, loss = 0.05387849\n",
      "Iteration 336, loss = 0.05350976\n",
      "Iteration 337, loss = 0.05315419\n",
      "Iteration 338, loss = 0.05281263\n",
      "Iteration 339, loss = 0.05245409\n",
      "Iteration 340, loss = 0.05213431\n",
      "Iteration 341, loss = 0.05178047\n",
      "Iteration 342, loss = 0.05146009\n",
      "Iteration 343, loss = 0.05111854\n",
      "Iteration 344, loss = 0.05081935\n",
      "Iteration 345, loss = 0.05049096\n",
      "Iteration 346, loss = 0.05012848\n",
      "Iteration 347, loss = 0.04982819\n",
      "Iteration 348, loss = 0.04950809\n",
      "Iteration 349, loss = 0.04920674\n",
      "Iteration 350, loss = 0.04891478\n",
      "Iteration 351, loss = 0.04855399\n",
      "Iteration 352, loss = 0.04826662\n",
      "Iteration 353, loss = 0.04797481\n",
      "Iteration 354, loss = 0.04765639\n",
      "Iteration 355, loss = 0.04737282\n",
      "Iteration 356, loss = 0.04702957\n",
      "Iteration 357, loss = 0.04670683\n",
      "Iteration 358, loss = 0.04642687\n",
      "Iteration 359, loss = 0.04614791\n",
      "Iteration 360, loss = 0.04584162\n",
      "Iteration 361, loss = 0.04555801\n",
      "Iteration 362, loss = 0.04530184\n",
      "Iteration 363, loss = 0.04499122\n",
      "Iteration 364, loss = 0.04474584\n",
      "Iteration 365, loss = 0.04442209\n",
      "Iteration 366, loss = 0.04417280\n",
      "Iteration 367, loss = 0.04391679\n",
      "Iteration 368, loss = 0.04364644\n",
      "Iteration 369, loss = 0.04337319\n",
      "Iteration 370, loss = 0.04314601\n",
      "Iteration 371, loss = 0.04283026\n",
      "Iteration 372, loss = 0.04257544\n",
      "Iteration 373, loss = 0.04233526\n",
      "Iteration 374, loss = 0.04206622\n",
      "Iteration 375, loss = 0.04183939\n",
      "Iteration 376, loss = 0.04158090\n",
      "Iteration 377, loss = 0.04134113\n",
      "Iteration 378, loss = 0.04109050\n",
      "Iteration 379, loss = 0.04086799\n",
      "Iteration 380, loss = 0.04062378\n",
      "Iteration 381, loss = 0.04040487\n",
      "Iteration 382, loss = 0.04012917\n",
      "Iteration 383, loss = 0.03991534\n",
      "Iteration 384, loss = 0.03967528\n",
      "Iteration 385, loss = 0.03942598\n",
      "Iteration 386, loss = 0.03921386\n",
      "Iteration 387, loss = 0.03894832\n",
      "Iteration 388, loss = 0.03872019\n",
      "Iteration 389, loss = 0.03850474\n",
      "Iteration 390, loss = 0.03831103\n",
      "Iteration 391, loss = 0.03804217\n",
      "Iteration 392, loss = 0.03781436\n",
      "Iteration 393, loss = 0.03758353\n",
      "Iteration 394, loss = 0.03746741\n",
      "Iteration 395, loss = 0.03714954\n",
      "Iteration 396, loss = 0.03692867\n",
      "Iteration 397, loss = 0.03672164\n",
      "Iteration 398, loss = 0.03649846\n",
      "Iteration 399, loss = 0.03630726\n",
      "Iteration 400, loss = 0.03608531\n",
      "Iteration 401, loss = 0.03588374\n",
      "Iteration 402, loss = 0.03572841\n",
      "Iteration 403, loss = 0.03548909\n",
      "Iteration 404, loss = 0.03528783\n",
      "Iteration 405, loss = 0.03508258\n",
      "Iteration 406, loss = 0.03488435\n",
      "Iteration 407, loss = 0.03468510\n",
      "Iteration 408, loss = 0.03448582\n",
      "Iteration 409, loss = 0.03429669\n",
      "Iteration 410, loss = 0.03410365\n",
      "Iteration 411, loss = 0.03391810\n",
      "Iteration 412, loss = 0.03376641\n",
      "Iteration 413, loss = 0.03354824\n",
      "Iteration 414, loss = 0.03340022\n",
      "Iteration 415, loss = 0.03326273\n",
      "Iteration 416, loss = 0.03304876\n",
      "Iteration 417, loss = 0.03287196\n",
      "Iteration 418, loss = 0.03272423\n",
      "Iteration 419, loss = 0.03252416\n",
      "Iteration 420, loss = 0.03235943\n",
      "Iteration 421, loss = 0.03220934\n",
      "Iteration 422, loss = 0.03201672\n",
      "Iteration 423, loss = 0.03185527\n",
      "Iteration 424, loss = 0.03168267\n",
      "Iteration 425, loss = 0.03152011\n",
      "Iteration 426, loss = 0.03136059\n",
      "Iteration 427, loss = 0.03118501\n",
      "Iteration 428, loss = 0.03104756\n",
      "Iteration 429, loss = 0.03087419\n",
      "Iteration 430, loss = 0.03070394\n",
      "Iteration 431, loss = 0.03054007\n",
      "Iteration 432, loss = 0.03040248\n",
      "Iteration 433, loss = 0.03023260\n",
      "Iteration 434, loss = 0.03008562\n",
      "Iteration 435, loss = 0.02993993\n",
      "Iteration 436, loss = 0.02983041\n",
      "Iteration 437, loss = 0.02963624\n",
      "Iteration 438, loss = 0.02949313\n",
      "Iteration 439, loss = 0.02933325\n",
      "Iteration 440, loss = 0.02918950\n",
      "Iteration 441, loss = 0.02904984\n",
      "Iteration 442, loss = 0.02890036\n",
      "Iteration 443, loss = 0.02873740\n",
      "Iteration 444, loss = 0.02862057\n",
      "Iteration 445, loss = 0.02847728\n",
      "Iteration 446, loss = 0.02833732\n",
      "Iteration 447, loss = 0.02818946\n",
      "Iteration 448, loss = 0.02806627\n",
      "Iteration 449, loss = 0.02792573\n",
      "Iteration 450, loss = 0.02781232\n",
      "Iteration 451, loss = 0.02769411\n",
      "Iteration 452, loss = 0.02754339\n",
      "Iteration 453, loss = 0.02737344\n",
      "Iteration 454, loss = 0.02730573\n",
      "Iteration 455, loss = 0.02711888\n",
      "Iteration 456, loss = 0.02703005\n",
      "Iteration 457, loss = 0.02690519\n",
      "Iteration 458, loss = 0.02674195\n",
      "Iteration 459, loss = 0.02662759\n",
      "Iteration 460, loss = 0.02650134\n",
      "Iteration 461, loss = 0.02637660\n",
      "Iteration 462, loss = 0.02625026\n",
      "Iteration 463, loss = 0.02612864\n",
      "Iteration 464, loss = 0.02601394\n",
      "Iteration 465, loss = 0.02588575\n",
      "Iteration 466, loss = 0.02577801\n",
      "Iteration 467, loss = 0.02565368\n",
      "Iteration 468, loss = 0.02555174\n",
      "Iteration 469, loss = 0.02541338\n",
      "Iteration 470, loss = 0.02534702\n",
      "Iteration 471, loss = 0.02521274\n",
      "Iteration 472, loss = 0.02510319\n",
      "Iteration 473, loss = 0.02498435\n",
      "Iteration 474, loss = 0.02490186\n",
      "Iteration 475, loss = 0.02474432\n",
      "Iteration 476, loss = 0.02465083\n",
      "Iteration 477, loss = 0.02453404\n",
      "Iteration 478, loss = 0.02443806\n",
      "Iteration 479, loss = 0.02432101\n",
      "Iteration 480, loss = 0.02421697\n",
      "Iteration 481, loss = 0.02412154\n",
      "Iteration 482, loss = 0.02400859\n",
      "Iteration 483, loss = 0.02392036\n",
      "Iteration 484, loss = 0.02380913\n",
      "Iteration 485, loss = 0.02372091\n",
      "Iteration 486, loss = 0.02360173\n",
      "Iteration 487, loss = 0.02351389\n",
      "Iteration 488, loss = 0.02341350\n",
      "Iteration 489, loss = 0.02330470\n",
      "Iteration 490, loss = 0.02325834\n",
      "Iteration 491, loss = 0.02314130\n",
      "Iteration 492, loss = 0.02304829\n",
      "Iteration 493, loss = 0.02292474\n",
      "Iteration 494, loss = 0.02281966\n",
      "Iteration 495, loss = 0.02272838\n",
      "Iteration 496, loss = 0.02265217\n",
      "Iteration 497, loss = 0.02254826\n",
      "Iteration 498, loss = 0.02248960\n",
      "Iteration 499, loss = 0.02237665\n",
      "Iteration 500, loss = 0.02229842\n",
      "Iteration 501, loss = 0.02220169\n",
      "Iteration 502, loss = 0.02211090\n",
      "Iteration 503, loss = 0.02202459\n",
      "Iteration 504, loss = 0.02195016\n",
      "Iteration 505, loss = 0.02185510\n",
      "Iteration 506, loss = 0.02180360\n",
      "Iteration 507, loss = 0.02170036\n",
      "Iteration 508, loss = 0.02159330\n",
      "Iteration 509, loss = 0.02149738\n",
      "Iteration 510, loss = 0.02141595\n",
      "Iteration 511, loss = 0.02134996\n",
      "Iteration 512, loss = 0.02123365\n",
      "Iteration 513, loss = 0.02119439\n",
      "Iteration 514, loss = 0.02108563\n",
      "Iteration 515, loss = 0.02101075\n",
      "Iteration 516, loss = 0.02097100\n",
      "Iteration 517, loss = 0.02085435\n",
      "Iteration 518, loss = 0.02076436\n",
      "Iteration 519, loss = 0.02067543\n",
      "Iteration 520, loss = 0.02060451\n",
      "Iteration 521, loss = 0.02049653\n",
      "Iteration 522, loss = 0.02045025\n",
      "Iteration 523, loss = 0.02033814\n",
      "Iteration 524, loss = 0.02028284\n",
      "Iteration 525, loss = 0.02017690\n",
      "Iteration 526, loss = 0.02010154\n",
      "Iteration 527, loss = 0.02001938\n",
      "Iteration 528, loss = 0.01998572\n",
      "Iteration 529, loss = 0.01987988\n",
      "Iteration 530, loss = 0.01978496\n",
      "Iteration 531, loss = 0.01973725\n",
      "Iteration 532, loss = 0.01966661\n",
      "Iteration 533, loss = 0.01956873\n",
      "Iteration 534, loss = 0.01950011\n",
      "Iteration 535, loss = 0.01940345\n",
      "Iteration 536, loss = 0.01934367\n",
      "Iteration 537, loss = 0.01928728\n",
      "Iteration 538, loss = 0.01917406\n",
      "Iteration 539, loss = 0.01913131\n",
      "Iteration 540, loss = 0.01904923\n",
      "Iteration 541, loss = 0.01898595\n",
      "Iteration 542, loss = 0.01890133\n",
      "Iteration 543, loss = 0.01884356\n",
      "Iteration 544, loss = 0.01876683\n",
      "Iteration 545, loss = 0.01870373\n",
      "Iteration 546, loss = 0.01862331\n",
      "Iteration 547, loss = 0.01856617\n",
      "Iteration 548, loss = 0.01852898\n",
      "Iteration 549, loss = 0.01846017\n",
      "Iteration 550, loss = 0.01836319\n",
      "Iteration 551, loss = 0.01828319\n",
      "Iteration 552, loss = 0.01822578\n",
      "Iteration 553, loss = 0.01816323\n",
      "Iteration 554, loss = 0.01811019\n",
      "Iteration 555, loss = 0.01802382\n",
      "Iteration 556, loss = 0.01796743\n",
      "Iteration 557, loss = 0.01792250\n",
      "Iteration 558, loss = 0.01787146\n",
      "Iteration 559, loss = 0.01778640\n",
      "Iteration 560, loss = 0.01772177\n",
      "Iteration 561, loss = 0.01766914\n",
      "Iteration 562, loss = 0.01759974\n",
      "Iteration 563, loss = 0.01750480\n",
      "Iteration 564, loss = 0.01745326\n",
      "Iteration 565, loss = 0.01740864\n",
      "Iteration 566, loss = 0.01734009\n",
      "Iteration 567, loss = 0.01728207\n",
      "Iteration 568, loss = 0.01719552\n",
      "Iteration 569, loss = 0.01714513\n",
      "Iteration 570, loss = 0.01709602\n",
      "Iteration 571, loss = 0.01703016\n",
      "Iteration 572, loss = 0.01697409\n",
      "Iteration 573, loss = 0.01689655\n",
      "Iteration 574, loss = 0.01685795\n",
      "Iteration 575, loss = 0.01677760\n",
      "Iteration 576, loss = 0.01672707\n",
      "Iteration 577, loss = 0.01668242\n",
      "Iteration 578, loss = 0.01661085\n",
      "Iteration 579, loss = 0.01656280\n",
      "Iteration 580, loss = 0.01648945\n",
      "Iteration 581, loss = 0.01645116\n",
      "Iteration 582, loss = 0.01641707\n",
      "Iteration 583, loss = 0.01632970\n",
      "Iteration 584, loss = 0.01627316\n",
      "Iteration 585, loss = 0.01625084\n",
      "Iteration 586, loss = 0.01618061\n",
      "Iteration 587, loss = 0.01610788\n",
      "Iteration 588, loss = 0.01611935\n",
      "Iteration 589, loss = 0.01600116\n",
      "Iteration 590, loss = 0.01597613\n",
      "Iteration 591, loss = 0.01590857\n",
      "Iteration 592, loss = 0.01587840\n",
      "Iteration 593, loss = 0.01580639\n",
      "Iteration 594, loss = 0.01581822\n",
      "Iteration 595, loss = 0.01572015\n",
      "Iteration 596, loss = 0.01565463\n",
      "Iteration 597, loss = 0.01560438\n",
      "Iteration 598, loss = 0.01555499\n",
      "Iteration 599, loss = 0.01550537\n",
      "Iteration 600, loss = 0.01547395\n",
      "Iteration 601, loss = 0.01539090\n",
      "Iteration 602, loss = 0.01534647\n",
      "Iteration 603, loss = 0.01531607\n",
      "Iteration 604, loss = 0.01527036\n",
      "Iteration 605, loss = 0.01519104\n",
      "Iteration 606, loss = 0.01513445\n",
      "Iteration 607, loss = 0.01508988\n",
      "Iteration 608, loss = 0.01510986\n",
      "Iteration 609, loss = 0.01501381\n",
      "Iteration 610, loss = 0.01498007\n",
      "Iteration 611, loss = 0.01491694\n",
      "Iteration 612, loss = 0.01485801\n",
      "Iteration 613, loss = 0.01480432\n",
      "Iteration 614, loss = 0.01479147\n",
      "Iteration 615, loss = 0.01471731\n",
      "Iteration 616, loss = 0.01465694\n",
      "Iteration 617, loss = 0.01461679\n",
      "Iteration 618, loss = 0.01457139\n",
      "Iteration 619, loss = 0.01455048\n",
      "Iteration 620, loss = 0.01450557\n",
      "Iteration 621, loss = 0.01444944\n",
      "Iteration 622, loss = 0.01440826\n",
      "Iteration 623, loss = 0.01435907\n",
      "Iteration 624, loss = 0.01433943\n",
      "Iteration 625, loss = 0.01428609\n",
      "Iteration 626, loss = 0.01423135\n",
      "Iteration 627, loss = 0.01419259\n",
      "Iteration 628, loss = 0.01416117\n",
      "Iteration 629, loss = 0.01410042\n",
      "Iteration 630, loss = 0.01407540\n",
      "Iteration 631, loss = 0.01403634\n",
      "Iteration 632, loss = 0.01397506\n",
      "Iteration 633, loss = 0.01391513\n",
      "Iteration 634, loss = 0.01388105\n",
      "Iteration 635, loss = 0.01390184\n",
      "Iteration 636, loss = 0.01380552\n",
      "Iteration 637, loss = 0.01377299\n",
      "Iteration 638, loss = 0.01371227\n",
      "Iteration 639, loss = 0.01373378\n",
      "Iteration 640, loss = 0.01363112\n",
      "Iteration 641, loss = 0.01357248\n",
      "Iteration 642, loss = 0.01354406\n",
      "Iteration 643, loss = 0.01350665\n",
      "Iteration 644, loss = 0.01345731\n",
      "Iteration 645, loss = 0.01341777\n",
      "Iteration 646, loss = 0.01338437\n",
      "Iteration 647, loss = 0.01334035\n",
      "Iteration 648, loss = 0.01329243\n",
      "Iteration 649, loss = 0.01328518\n",
      "Iteration 650, loss = 0.01330818\n",
      "Iteration 651, loss = 0.01319168\n",
      "Iteration 652, loss = 0.01316982\n",
      "Iteration 653, loss = 0.01310255\n",
      "Iteration 654, loss = 0.01311341\n",
      "Iteration 655, loss = 0.01307425\n",
      "Iteration 656, loss = 0.01299244\n",
      "Iteration 657, loss = 0.01297445\n",
      "Iteration 658, loss = 0.01291427\n",
      "Iteration 659, loss = 0.01287526\n",
      "Iteration 660, loss = 0.01284106\n",
      "Iteration 661, loss = 0.01280078\n",
      "Iteration 662, loss = 0.01278111\n",
      "Iteration 663, loss = 0.01274696\n",
      "Iteration 664, loss = 0.01269675\n",
      "Iteration 665, loss = 0.01265317\n",
      "Iteration 666, loss = 0.01262897\n",
      "Iteration 667, loss = 0.01257594\n",
      "Iteration 668, loss = 0.01256328\n",
      "Iteration 669, loss = 0.01252567\n",
      "Iteration 670, loss = 0.01245891\n",
      "Iteration 671, loss = 0.01250111\n",
      "Iteration 672, loss = 0.01246183\n",
      "Iteration 673, loss = 0.01235798\n",
      "Iteration 674, loss = 0.01232483\n",
      "Iteration 675, loss = 0.01228772\n",
      "Iteration 676, loss = 0.01225969\n",
      "Iteration 677, loss = 0.01222415\n",
      "Iteration 678, loss = 0.01219729\n",
      "Iteration 679, loss = 0.01215608\n",
      "Iteration 680, loss = 0.01212228\n",
      "Iteration 681, loss = 0.01208799\n",
      "Iteration 682, loss = 0.01206309\n",
      "Iteration 683, loss = 0.01204235\n",
      "Iteration 684, loss = 0.01197395\n",
      "Iteration 685, loss = 0.01193302\n",
      "Iteration 686, loss = 0.01190396\n",
      "Iteration 687, loss = 0.01186479\n",
      "Iteration 688, loss = 0.01184812\n",
      "Iteration 689, loss = 0.01179002\n",
      "Iteration 690, loss = 0.01177434\n",
      "Iteration 691, loss = 0.01174330\n",
      "Iteration 692, loss = 0.01171240\n",
      "Iteration 693, loss = 0.01166307\n",
      "Iteration 694, loss = 0.01164295\n",
      "Iteration 695, loss = 0.01158631\n",
      "Iteration 696, loss = 0.01158577\n",
      "Iteration 697, loss = 0.01151831\n",
      "Iteration 698, loss = 0.01149193\n",
      "Iteration 699, loss = 0.01146092\n",
      "Iteration 700, loss = 0.01144156\n",
      "Iteration 701, loss = 0.01145214\n",
      "Iteration 702, loss = 0.01140582\n",
      "Iteration 703, loss = 0.01135757\n",
      "Iteration 704, loss = 0.01135494\n",
      "Iteration 705, loss = 0.01128513\n",
      "Iteration 706, loss = 0.01123040\n",
      "Iteration 707, loss = 0.01120147\n",
      "Iteration 708, loss = 0.01119295\n",
      "Iteration 709, loss = 0.01117141\n",
      "Iteration 710, loss = 0.01112133\n",
      "Iteration 711, loss = 0.01107442\n",
      "Iteration 712, loss = 0.01107450\n",
      "Iteration 713, loss = 0.01104042\n",
      "Iteration 714, loss = 0.01099359\n",
      "Iteration 715, loss = 0.01097762\n",
      "Iteration 716, loss = 0.01095929\n",
      "Iteration 717, loss = 0.01092849\n",
      "Iteration 718, loss = 0.01088592\n",
      "Iteration 719, loss = 0.01084430\n",
      "Iteration 720, loss = 0.01080948\n",
      "Iteration 721, loss = 0.01082080\n",
      "Iteration 722, loss = 0.01075547\n",
      "Iteration 723, loss = 0.01071556\n",
      "Iteration 724, loss = 0.01070213\n",
      "Iteration 725, loss = 0.01066074\n",
      "Iteration 726, loss = 0.01063931\n",
      "Iteration 727, loss = 0.01059994\n",
      "Iteration 728, loss = 0.01058058\n",
      "Iteration 729, loss = 0.01053142\n",
      "Iteration 730, loss = 0.01053030\n",
      "Iteration 731, loss = 0.01049233\n",
      "Iteration 732, loss = 0.01046721\n",
      "Iteration 733, loss = 0.01045746\n",
      "Iteration 734, loss = 0.01040377\n",
      "Iteration 735, loss = 0.01038585\n",
      "Iteration 736, loss = 0.01036522\n",
      "Iteration 737, loss = 0.01030863\n",
      "Iteration 738, loss = 0.01028687\n",
      "Iteration 739, loss = 0.01025286\n",
      "Iteration 740, loss = 0.01022771\n",
      "Iteration 741, loss = 0.01020891\n",
      "Iteration 742, loss = 0.01019268\n",
      "Iteration 743, loss = 0.01013775\n",
      "Iteration 744, loss = 0.01012125\n",
      "Iteration 745, loss = 0.01007916\n",
      "Iteration 746, loss = 0.01005917\n",
      "Iteration 747, loss = 0.01006051\n",
      "Iteration 748, loss = 0.01004714\n",
      "Iteration 749, loss = 0.00998178\n",
      "Iteration 750, loss = 0.00996569\n",
      "Iteration 751, loss = 0.00992236\n",
      "Iteration 752, loss = 0.00991542\n",
      "Iteration 753, loss = 0.00989081\n",
      "Iteration 754, loss = 0.00988242\n",
      "Iteration 755, loss = 0.00982645\n",
      "Iteration 756, loss = 0.00981890\n",
      "Iteration 757, loss = 0.00976852\n",
      "Iteration 758, loss = 0.00974174\n",
      "Iteration 759, loss = 0.00972541\n",
      "Iteration 760, loss = 0.00968459\n",
      "Iteration 761, loss = 0.00967309\n",
      "Iteration 762, loss = 0.00963497\n",
      "Iteration 763, loss = 0.00961965\n",
      "Iteration 764, loss = 0.00961476\n",
      "Iteration 765, loss = 0.00961265\n",
      "Iteration 766, loss = 0.00953938\n",
      "Iteration 767, loss = 0.00949452\n",
      "Iteration 768, loss = 0.00949300\n",
      "Iteration 769, loss = 0.00945982\n",
      "Iteration 770, loss = 0.00944240\n",
      "Iteration 771, loss = 0.00940381\n",
      "Iteration 772, loss = 0.00938538\n",
      "Iteration 773, loss = 0.00935275\n",
      "Iteration 774, loss = 0.00934345\n",
      "Iteration 775, loss = 0.00930304\n",
      "Iteration 776, loss = 0.00930192\n",
      "Iteration 777, loss = 0.00926948\n",
      "Iteration 778, loss = 0.00923649\n",
      "Iteration 779, loss = 0.00922587\n",
      "Iteration 780, loss = 0.00920469\n",
      "Iteration 781, loss = 0.00915688\n",
      "Iteration 782, loss = 0.00911831\n",
      "Iteration 783, loss = 0.00913475\n",
      "Iteration 784, loss = 0.00910063\n",
      "Iteration 785, loss = 0.00909763\n",
      "Iteration 786, loss = 0.00904703\n",
      "Iteration 787, loss = 0.00901988\n",
      "Iteration 788, loss = 0.00898059\n",
      "Iteration 789, loss = 0.00896861\n",
      "Iteration 790, loss = 0.00893365\n",
      "Iteration 791, loss = 0.00895608\n",
      "Iteration 792, loss = 0.00892422\n",
      "Iteration 793, loss = 0.00888109\n",
      "Iteration 794, loss = 0.00885962\n",
      "Iteration 795, loss = 0.00884415\n",
      "Iteration 796, loss = 0.00883139\n",
      "Iteration 797, loss = 0.00883386\n",
      "Iteration 798, loss = 0.00882685\n",
      "Iteration 799, loss = 0.00875458\n",
      "Iteration 800, loss = 0.00872906\n",
      "Iteration 801, loss = 0.00870670\n",
      "Iteration 802, loss = 0.00869169\n",
      "Iteration 803, loss = 0.00864582\n",
      "Iteration 804, loss = 0.00866574\n",
      "Iteration 805, loss = 0.00860917\n",
      "Iteration 806, loss = 0.00860261\n",
      "Iteration 807, loss = 0.00860198\n",
      "Iteration 808, loss = 0.00855494\n",
      "Iteration 809, loss = 0.00852974\n",
      "Iteration 810, loss = 0.00852393\n",
      "Iteration 811, loss = 0.00849075\n",
      "Iteration 812, loss = 0.00848417\n",
      "Iteration 813, loss = 0.00844404\n",
      "Iteration 814, loss = 0.00841925\n",
      "Iteration 815, loss = 0.00839339\n",
      "Iteration 816, loss = 0.00840402\n",
      "Iteration 817, loss = 0.00832935\n",
      "Iteration 818, loss = 0.00833557\n",
      "Iteration 819, loss = 0.00830771\n",
      "Iteration 820, loss = 0.00829526\n",
      "Iteration 821, loss = 0.00828291\n",
      "Iteration 822, loss = 0.00828994\n",
      "Iteration 823, loss = 0.00821569\n",
      "Iteration 824, loss = 0.00820568\n",
      "Iteration 825, loss = 0.00818467\n",
      "Iteration 826, loss = 0.00815853\n",
      "Iteration 827, loss = 0.00814122\n",
      "Iteration 828, loss = 0.00810334\n",
      "Iteration 829, loss = 0.00808569\n",
      "Iteration 830, loss = 0.00808345\n",
      "Iteration 831, loss = 0.00807556\n",
      "Iteration 832, loss = 0.00802807\n",
      "Iteration 833, loss = 0.00803494\n",
      "Iteration 834, loss = 0.00802532\n",
      "Iteration 835, loss = 0.00800363\n",
      "Iteration 836, loss = 0.00796729\n",
      "Iteration 837, loss = 0.00794179\n",
      "Iteration 838, loss = 0.00794571\n",
      "Iteration 839, loss = 0.00791346\n",
      "Iteration 840, loss = 0.00789075\n",
      "Iteration 841, loss = 0.00787258\n",
      "Iteration 842, loss = 0.00785905\n",
      "Iteration 843, loss = 0.00781161\n",
      "Iteration 844, loss = 0.00780615\n",
      "Iteration 845, loss = 0.00784869\n",
      "Iteration 846, loss = 0.00782248\n",
      "Iteration 847, loss = 0.00774069\n",
      "Iteration 848, loss = 0.00771504\n",
      "Iteration 849, loss = 0.00773850\n",
      "Iteration 850, loss = 0.00768268\n",
      "Iteration 851, loss = 0.00766325\n",
      "Iteration 852, loss = 0.00767994\n",
      "Iteration 853, loss = 0.00763150\n",
      "Iteration 854, loss = 0.00759658\n",
      "Iteration 855, loss = 0.00758231\n",
      "Iteration 856, loss = 0.00765156\n",
      "Iteration 857, loss = 0.00756369\n",
      "Iteration 858, loss = 0.00753312\n",
      "Iteration 859, loss = 0.00752811\n",
      "Iteration 860, loss = 0.00749493\n",
      "Iteration 861, loss = 0.00750322\n",
      "Iteration 862, loss = 0.00746261\n",
      "Iteration 863, loss = 0.00746442\n",
      "Iteration 864, loss = 0.00743910\n",
      "Iteration 865, loss = 0.00740825\n",
      "Iteration 866, loss = 0.00738895\n",
      "Iteration 867, loss = 0.00737709\n",
      "Iteration 868, loss = 0.00735875\n",
      "Iteration 869, loss = 0.00732320\n",
      "Iteration 870, loss = 0.00731963\n",
      "Iteration 871, loss = 0.00738129\n",
      "Iteration 872, loss = 0.00729453\n",
      "Iteration 873, loss = 0.00727049\n",
      "Iteration 874, loss = 0.00724711\n",
      "Iteration 875, loss = 0.00724830\n",
      "Iteration 876, loss = 0.00722641\n",
      "Iteration 877, loss = 0.00721237\n",
      "Iteration 878, loss = 0.00718793\n",
      "Iteration 879, loss = 0.00715682\n",
      "Iteration 880, loss = 0.00715500\n",
      "Iteration 881, loss = 0.00713021\n",
      "Iteration 882, loss = 0.00712286\n",
      "Iteration 883, loss = 0.00712401\n",
      "Iteration 884, loss = 0.00707749\n",
      "Iteration 885, loss = 0.00707765\n",
      "Iteration 886, loss = 0.00707042\n",
      "Iteration 887, loss = 0.00702535\n",
      "Iteration 888, loss = 0.00702437\n",
      "Iteration 889, loss = 0.00702555\n",
      "Iteration 890, loss = 0.00696729\n",
      "Iteration 891, loss = 0.00696332\n",
      "Iteration 892, loss = 0.00695964\n",
      "Iteration 893, loss = 0.00693175\n",
      "Iteration 894, loss = 0.00690797\n",
      "Iteration 895, loss = 0.00690887\n",
      "Iteration 896, loss = 0.00690252\n",
      "Iteration 897, loss = 0.00685777\n",
      "Iteration 898, loss = 0.00685657\n",
      "Iteration 899, loss = 0.00684312\n",
      "Iteration 900, loss = 0.00684574\n",
      "Iteration 901, loss = 0.00682597\n",
      "Iteration 902, loss = 0.00683130\n",
      "Iteration 903, loss = 0.00677923\n",
      "Iteration 904, loss = 0.00677295\n",
      "Iteration 905, loss = 0.00673698\n",
      "Iteration 906, loss = 0.00672968\n",
      "Iteration 907, loss = 0.00672339\n",
      "Iteration 908, loss = 0.00668281\n",
      "Iteration 909, loss = 0.00667601\n",
      "Iteration 910, loss = 0.00665826\n",
      "Iteration 911, loss = 0.00667378\n",
      "Iteration 912, loss = 0.00662871\n",
      "Iteration 913, loss = 0.00662687\n",
      "Iteration 914, loss = 0.00660813\n",
      "Iteration 915, loss = 0.00659117\n",
      "Iteration 916, loss = 0.00656351\n",
      "Iteration 917, loss = 0.00657292\n",
      "Iteration 918, loss = 0.00652934\n",
      "Iteration 919, loss = 0.00653326\n",
      "Iteration 920, loss = 0.00653446\n",
      "Iteration 921, loss = 0.00654239\n",
      "Iteration 922, loss = 0.00649141\n",
      "Iteration 923, loss = 0.00645800\n",
      "Iteration 924, loss = 0.00644958\n",
      "Iteration 925, loss = 0.00644078\n",
      "Iteration 926, loss = 0.00642136\n",
      "Iteration 927, loss = 0.00640251\n",
      "Iteration 928, loss = 0.00639675\n",
      "Iteration 929, loss = 0.00637779\n",
      "Iteration 930, loss = 0.00637901\n",
      "Iteration 931, loss = 0.00634036\n",
      "Iteration 932, loss = 0.00632668\n",
      "Iteration 933, loss = 0.00630417\n",
      "Iteration 934, loss = 0.00630884\n",
      "Iteration 935, loss = 0.00629213\n",
      "Iteration 936, loss = 0.00629467\n",
      "Iteration 937, loss = 0.00628367\n",
      "Iteration 938, loss = 0.00626317\n",
      "Iteration 939, loss = 0.00626673\n",
      "Iteration 940, loss = 0.00621009\n",
      "Iteration 941, loss = 0.00621851\n",
      "Iteration 942, loss = 0.00617644\n",
      "Iteration 943, loss = 0.00619021\n",
      "Iteration 944, loss = 0.00617090\n",
      "Iteration 945, loss = 0.00614624\n",
      "Iteration 946, loss = 0.00612305\n",
      "Iteration 947, loss = 0.00610564\n",
      "Iteration 948, loss = 0.00609351\n",
      "Iteration 949, loss = 0.00610897\n",
      "Iteration 950, loss = 0.00607276\n",
      "Iteration 951, loss = 0.00605389\n",
      "Iteration 952, loss = 0.00604681\n",
      "Iteration 953, loss = 0.00602203\n",
      "Iteration 954, loss = 0.00600132\n",
      "Iteration 955, loss = 0.00601178\n",
      "Iteration 956, loss = 0.00598204\n",
      "Iteration 957, loss = 0.00598216\n",
      "Iteration 958, loss = 0.00593654\n",
      "Iteration 959, loss = 0.00594456\n",
      "Iteration 960, loss = 0.00592282\n",
      "Iteration 961, loss = 0.00589845\n",
      "Iteration 962, loss = 0.00591326\n",
      "Iteration 963, loss = 0.00588904\n",
      "Iteration 964, loss = 0.00586390\n",
      "Iteration 965, loss = 0.00586679\n",
      "Iteration 966, loss = 0.00584072\n",
      "Iteration 967, loss = 0.00583384\n",
      "Iteration 968, loss = 0.00583756\n",
      "Iteration 969, loss = 0.00580042\n",
      "Iteration 970, loss = 0.00576849\n",
      "Iteration 971, loss = 0.00579121\n",
      "Iteration 972, loss = 0.00576201\n",
      "Iteration 973, loss = 0.00572223\n",
      "Iteration 974, loss = 0.00573746\n",
      "Iteration 975, loss = 0.00570189\n",
      "Iteration 976, loss = 0.00569659\n",
      "Iteration 977, loss = 0.00571638\n",
      "Iteration 978, loss = 0.00569258\n",
      "Iteration 979, loss = 0.00564468\n",
      "Iteration 980, loss = 0.00566407\n",
      "Iteration 981, loss = 0.00562943\n",
      "Iteration 982, loss = 0.00561818\n",
      "Iteration 983, loss = 0.00561010\n",
      "Iteration 984, loss = 0.00558608\n",
      "Iteration 985, loss = 0.00557860\n",
      "Iteration 986, loss = 0.00555875\n",
      "Iteration 987, loss = 0.00556340\n",
      "Iteration 988, loss = 0.00551895\n",
      "Iteration 989, loss = 0.00551579\n",
      "Iteration 990, loss = 0.00548546\n",
      "Iteration 991, loss = 0.00550051\n",
      "Iteration 992, loss = 0.00546870\n",
      "Iteration 993, loss = 0.00548681\n",
      "Iteration 994, loss = 0.00542658\n",
      "Iteration 995, loss = 0.00545154\n",
      "Iteration 996, loss = 0.00542149\n",
      "Iteration 997, loss = 0.00541658\n",
      "Iteration 998, loss = 0.00540043\n",
      "Iteration 999, loss = 0.00542969\n",
      "Iteration 1000, loss = 0.00538589\n",
      "Iteration 1001, loss = 0.00536125\n",
      "Iteration 1002, loss = 0.00535850\n",
      "Iteration 1003, loss = 0.00535564\n",
      "Iteration 1004, loss = 0.00532086\n",
      "Iteration 1005, loss = 0.00529348\n",
      "Iteration 1006, loss = 0.00538109\n",
      "Iteration 1007, loss = 0.00529831\n",
      "Iteration 1008, loss = 0.00531761\n",
      "Iteration 1009, loss = 0.00527837\n",
      "Iteration 1010, loss = 0.00524479\n",
      "Iteration 1011, loss = 0.00526349\n",
      "Iteration 1012, loss = 0.00525710\n",
      "Iteration 1013, loss = 0.00522377\n",
      "Iteration 1014, loss = 0.00520878\n",
      "Iteration 1015, loss = 0.00517058\n",
      "Iteration 1016, loss = 0.00520689\n",
      "Iteration 1017, loss = 0.00516844\n",
      "Iteration 1018, loss = 0.00515993\n",
      "Iteration 1019, loss = 0.00515050\n",
      "Iteration 1020, loss = 0.00511858\n",
      "Iteration 1021, loss = 0.00513551\n",
      "Iteration 1022, loss = 0.00510570\n",
      "Iteration 1023, loss = 0.00511357\n",
      "Iteration 1024, loss = 0.00507636\n",
      "Iteration 1025, loss = 0.00507061\n",
      "Iteration 1026, loss = 0.00506782\n",
      "Iteration 1027, loss = 0.00508166\n",
      "Iteration 1028, loss = 0.00504227\n",
      "Iteration 1029, loss = 0.00503346\n",
      "Iteration 1030, loss = 0.00504986\n",
      "Iteration 1031, loss = 0.00499143\n",
      "Iteration 1032, loss = 0.00499214\n",
      "Iteration 1033, loss = 0.00500612\n",
      "Iteration 1034, loss = 0.00496127\n",
      "Iteration 1035, loss = 0.00497291\n",
      "Iteration 1036, loss = 0.00495427\n",
      "Iteration 1037, loss = 0.00492040\n",
      "Iteration 1038, loss = 0.00493448\n",
      "Iteration 1039, loss = 0.00490851\n",
      "Iteration 1040, loss = 0.00489386\n",
      "Iteration 1041, loss = 0.00489676\n",
      "Iteration 1042, loss = 0.00489685\n",
      "Iteration 1043, loss = 0.00487299\n",
      "Iteration 1044, loss = 0.00488975\n",
      "Iteration 1045, loss = 0.00485735\n",
      "Iteration 1046, loss = 0.00486174\n",
      "Iteration 1047, loss = 0.00482672\n",
      "Iteration 1048, loss = 0.00482442\n",
      "Iteration 1049, loss = 0.00482118\n",
      "Iteration 1050, loss = 0.00480026\n",
      "Iteration 1051, loss = 0.00479130\n",
      "Iteration 1052, loss = 0.00484054\n",
      "Iteration 1053, loss = 0.00476369\n",
      "Iteration 1054, loss = 0.00478719\n",
      "Iteration 1055, loss = 0.00473218\n",
      "Iteration 1056, loss = 0.00472775\n",
      "Iteration 1057, loss = 0.00471237\n",
      "Iteration 1058, loss = 0.00470685\n",
      "Iteration 1059, loss = 0.00469620\n",
      "Iteration 1060, loss = 0.00468715\n",
      "Iteration 1061, loss = 0.00466225\n",
      "Iteration 1062, loss = 0.00467789\n",
      "Iteration 1063, loss = 0.00467350\n",
      "Iteration 1064, loss = 0.00464793\n",
      "Iteration 1065, loss = 0.00463236\n",
      "Iteration 1066, loss = 0.00463030\n",
      "Iteration 1067, loss = 0.00463684\n",
      "Iteration 1068, loss = 0.00461229\n",
      "Iteration 1069, loss = 0.00460384\n",
      "Iteration 1070, loss = 0.00459681\n",
      "Iteration 1071, loss = 0.00457085\n",
      "Iteration 1072, loss = 0.00462681\n",
      "Iteration 1073, loss = 0.00456154\n",
      "Iteration 1074, loss = 0.00453573\n",
      "Iteration 1075, loss = 0.00454389\n",
      "Iteration 1076, loss = 0.00453565\n",
      "Iteration 1077, loss = 0.00454268\n",
      "Iteration 1078, loss = 0.00451053\n",
      "Iteration 1079, loss = 0.00450286\n",
      "Iteration 1080, loss = 0.00448498\n",
      "Iteration 1081, loss = 0.00447663\n",
      "Iteration 1082, loss = 0.00445512\n",
      "Iteration 1083, loss = 0.00445120\n",
      "Iteration 1084, loss = 0.00446017\n",
      "Iteration 1085, loss = 0.00445311\n",
      "Iteration 1086, loss = 0.00444313\n",
      "Iteration 1087, loss = 0.00443579\n",
      "Iteration 1088, loss = 0.00442850\n",
      "Iteration 1089, loss = 0.00445292\n",
      "Iteration 1090, loss = 0.00441172\n",
      "Iteration 1091, loss = 0.00440961\n",
      "Iteration 1092, loss = 0.00438120\n",
      "Iteration 1093, loss = 0.00438001\n",
      "Iteration 1094, loss = 0.00439358\n",
      "Iteration 1095, loss = 0.00436284\n",
      "Iteration 1096, loss = 0.00434730\n",
      "Iteration 1097, loss = 0.00438644\n",
      "Iteration 1098, loss = 0.00431111\n",
      "Iteration 1099, loss = 0.00433316\n",
      "Iteration 1100, loss = 0.00432868\n",
      "Iteration 1101, loss = 0.00430157\n",
      "Iteration 1102, loss = 0.00430264\n",
      "Iteration 1103, loss = 0.00429071\n",
      "Iteration 1104, loss = 0.00428734\n",
      "Iteration 1105, loss = 0.00427151\n",
      "Iteration 1106, loss = 0.00424996\n",
      "Iteration 1107, loss = 0.00425319\n",
      "Iteration 1108, loss = 0.00424470\n",
      "Iteration 1109, loss = 0.00422384\n",
      "Iteration 1110, loss = 0.00421321\n",
      "Iteration 1111, loss = 0.00420852\n",
      "Iteration 1112, loss = 0.00423076\n",
      "Iteration 1113, loss = 0.00419578\n",
      "Iteration 1114, loss = 0.00419805\n",
      "Iteration 1115, loss = 0.00421069\n",
      "Iteration 1116, loss = 0.00415044\n",
      "Iteration 1117, loss = 0.00419631\n",
      "Iteration 1118, loss = 0.00416753\n",
      "Iteration 1119, loss = 0.00415021\n",
      "Iteration 1120, loss = 0.00418129\n",
      "Iteration 1121, loss = 0.00411759\n",
      "Iteration 1122, loss = 0.00412361\n",
      "Iteration 1123, loss = 0.00409320\n",
      "Iteration 1124, loss = 0.00412332\n",
      "Iteration 1125, loss = 0.00409271\n",
      "Iteration 1126, loss = 0.00410160\n",
      "Iteration 1127, loss = 0.00407865\n",
      "Iteration 1128, loss = 0.00406734\n",
      "Iteration 1129, loss = 0.00405841\n",
      "Iteration 1130, loss = 0.00406019\n",
      "Iteration 1131, loss = 0.00407748\n",
      "Iteration 1132, loss = 0.00408126\n",
      "Iteration 1133, loss = 0.00405148\n",
      "Iteration 1134, loss = 0.00402429\n",
      "Iteration 1135, loss = 0.00401026\n",
      "Iteration 1136, loss = 0.00401472\n",
      "Iteration 1137, loss = 0.00399482\n",
      "Iteration 1138, loss = 0.00399124\n",
      "Iteration 1139, loss = 0.00400433\n",
      "Iteration 1140, loss = 0.00397688\n",
      "Iteration 1141, loss = 0.00398478\n",
      "Iteration 1142, loss = 0.00397245\n",
      "Iteration 1143, loss = 0.00398461\n",
      "Iteration 1144, loss = 0.00392255\n",
      "Iteration 1145, loss = 0.00392997\n",
      "Iteration 1146, loss = 0.00397935\n",
      "Iteration 1147, loss = 0.00393609\n",
      "Iteration 1148, loss = 0.00393051\n",
      "Iteration 1149, loss = 0.00393922\n",
      "Iteration 1150, loss = 0.00388225\n",
      "Iteration 1151, loss = 0.00389182\n",
      "Iteration 1152, loss = 0.00389739\n",
      "Iteration 1153, loss = 0.00390068\n",
      "Iteration 1154, loss = 0.00386792\n",
      "Iteration 1155, loss = 0.00391543\n",
      "Iteration 1156, loss = 0.00386500\n",
      "Iteration 1157, loss = 0.00383556\n",
      "Iteration 1158, loss = 0.00383884\n",
      "Iteration 1159, loss = 0.00383840\n",
      "Iteration 1160, loss = 0.00382588\n",
      "Iteration 1161, loss = 0.00381755\n",
      "Iteration 1162, loss = 0.00385118\n",
      "Iteration 1163, loss = 0.00380837\n",
      "Iteration 1164, loss = 0.00379762\n",
      "Iteration 1165, loss = 0.00379780\n",
      "Iteration 1166, loss = 0.00376746\n",
      "Iteration 1167, loss = 0.00382264\n",
      "Iteration 1168, loss = 0.00376756\n",
      "Iteration 1169, loss = 0.00375155\n",
      "Iteration 1170, loss = 0.00374112\n",
      "Iteration 1171, loss = 0.00375379\n",
      "Iteration 1172, loss = 0.00373539\n",
      "Iteration 1173, loss = 0.00374658\n",
      "Iteration 1174, loss = 0.00371443\n",
      "Iteration 1175, loss = 0.00372436\n",
      "Iteration 1176, loss = 0.00373864\n",
      "Iteration 1177, loss = 0.00371864\n",
      "Iteration 1178, loss = 0.00371413\n",
      "Iteration 1179, loss = 0.00367978\n",
      "Iteration 1180, loss = 0.00368671\n",
      "Iteration 1181, loss = 0.00367902\n",
      "Iteration 1182, loss = 0.00366823\n",
      "Iteration 1183, loss = 0.00366611\n",
      "Iteration 1184, loss = 0.00366282\n",
      "Iteration 1185, loss = 0.00364919\n",
      "Iteration 1186, loss = 0.00363776\n",
      "Iteration 1187, loss = 0.00363885\n",
      "Iteration 1188, loss = 0.00363340\n",
      "Iteration 1189, loss = 0.00363091\n",
      "Iteration 1190, loss = 0.00363130\n",
      "Iteration 1191, loss = 0.00361438\n",
      "Iteration 1192, loss = 0.00364490\n",
      "Iteration 1193, loss = 0.00362750\n",
      "Iteration 1194, loss = 0.00357739\n",
      "Iteration 1195, loss = 0.00358125\n",
      "Iteration 1196, loss = 0.00359398\n",
      "Iteration 1197, loss = 0.00355708\n",
      "Iteration 1198, loss = 0.00356875\n",
      "Iteration 1199, loss = 0.00357001\n",
      "Iteration 1200, loss = 0.00356281\n",
      "Iteration 1201, loss = 0.00355437\n",
      "Iteration 1202, loss = 0.00356792\n",
      "Iteration 1203, loss = 0.00353602\n",
      "Iteration 1204, loss = 0.00353519\n",
      "Iteration 1205, loss = 0.00353163\n",
      "Iteration 1206, loss = 0.00350776\n",
      "Iteration 1207, loss = 0.00350281\n",
      "Iteration 1208, loss = 0.00349516\n",
      "Iteration 1209, loss = 0.00350708\n",
      "Iteration 1210, loss = 0.00349440\n",
      "Iteration 1211, loss = 0.00349446\n",
      "Iteration 1212, loss = 0.00347020\n",
      "Iteration 1213, loss = 0.00347895\n",
      "Iteration 1214, loss = 0.00345647\n",
      "Iteration 1215, loss = 0.00348716\n",
      "Iteration 1216, loss = 0.00347289\n",
      "Iteration 1217, loss = 0.00344479\n",
      "Iteration 1218, loss = 0.00347732\n",
      "Iteration 1219, loss = 0.00344509\n",
      "Iteration 1220, loss = 0.00344098\n",
      "Iteration 1221, loss = 0.00343823\n",
      "Iteration 1222, loss = 0.00342776\n",
      "Iteration 1223, loss = 0.00341329\n",
      "Iteration 1224, loss = 0.00340046\n",
      "Iteration 1225, loss = 0.00341085\n",
      "Iteration 1226, loss = 0.00339744\n",
      "Iteration 1227, loss = 0.00339226\n",
      "Iteration 1228, loss = 0.00338998\n",
      "Iteration 1229, loss = 0.00337589\n",
      "Iteration 1230, loss = 0.00337274\n",
      "Iteration 1231, loss = 0.00335821\n",
      "Iteration 1232, loss = 0.00336861\n",
      "Iteration 1233, loss = 0.00333845\n",
      "Iteration 1234, loss = 0.00334826\n",
      "Iteration 1235, loss = 0.00333963\n",
      "Iteration 1236, loss = 0.00333378\n",
      "Iteration 1237, loss = 0.00333901\n",
      "Iteration 1238, loss = 0.00333705\n",
      "Iteration 1239, loss = 0.00331985\n",
      "Iteration 1240, loss = 0.00331296\n",
      "Iteration 1241, loss = 0.00333210\n",
      "Iteration 1242, loss = 0.00330290\n",
      "Iteration 1243, loss = 0.00333014\n",
      "Iteration 1244, loss = 0.00329480\n",
      "Iteration 1245, loss = 0.00330831\n",
      "Iteration 1246, loss = 0.00329387\n",
      "Iteration 1247, loss = 0.00328584\n",
      "Iteration 1248, loss = 0.00327714\n",
      "Iteration 1249, loss = 0.00328897\n",
      "Iteration 1250, loss = 0.00325188\n",
      "Iteration 1251, loss = 0.00324387\n",
      "Iteration 1252, loss = 0.00324769\n",
      "Iteration 1253, loss = 0.00325475\n",
      "Iteration 1254, loss = 0.00324121\n",
      "Iteration 1255, loss = 0.00322512\n",
      "Iteration 1256, loss = 0.00322226\n",
      "Iteration 1257, loss = 0.00321711\n",
      "Iteration 1258, loss = 0.00323290\n",
      "Iteration 1259, loss = 0.00319854\n",
      "Iteration 1260, loss = 0.00321269\n",
      "Iteration 1261, loss = 0.00319886\n",
      "Iteration 1262, loss = 0.00321166\n",
      "Iteration 1263, loss = 0.00319685\n",
      "Iteration 1264, loss = 0.00319531\n",
      "Iteration 1265, loss = 0.00320588\n",
      "Iteration 1266, loss = 0.00318043\n",
      "Iteration 1267, loss = 0.00316487\n",
      "Iteration 1268, loss = 0.00316053\n",
      "Iteration 1269, loss = 0.00317019\n",
      "Iteration 1270, loss = 0.00314867\n",
      "Iteration 1271, loss = 0.00318024\n",
      "Iteration 1272, loss = 0.00317385\n",
      "Iteration 1273, loss = 0.00313552\n",
      "Iteration 1274, loss = 0.00314410\n",
      "Iteration 1275, loss = 0.00314820\n",
      "Iteration 1276, loss = 0.00311753\n",
      "Iteration 1277, loss = 0.00312689\n",
      "Iteration 1278, loss = 0.00310600\n",
      "Iteration 1279, loss = 0.00315162\n",
      "Iteration 1280, loss = 0.00314027\n",
      "Iteration 1281, loss = 0.00310323\n",
      "Iteration 1282, loss = 0.00310134\n",
      "Iteration 1283, loss = 0.00311421\n",
      "Iteration 1284, loss = 0.00310881\n",
      "Iteration 1285, loss = 0.00309310\n",
      "Iteration 1286, loss = 0.00309924\n",
      "Iteration 1287, loss = 0.00309073\n",
      "Iteration 1288, loss = 0.00308899\n",
      "Iteration 1289, loss = 0.00309592\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_credit = MLPClassifier(max_iter = 1500, \n",
    "                              verbose = True, # True show the loss for each iteration\n",
    "                              tol = 0.0000100,\n",
    "                              solver = 'adam',\n",
    "                              activation = 'relu',\n",
    "                              hidden_layer_sizes = (2,2)# 3 neurons input -> 2 hidden -> 2 hidden -> 1 neuron output\n",
    "                                ) \n",
    "neural_credit.fit(x_credit_training,y_credit_training)\n",
    "prediction = neural_credit.predict(x_credit_test)\n",
    "accuracy_score(y_credit_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.998"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from yellowbrick.classifier import ConfusionMatrix\n",
    "confusion_matrix = ConfusionMatrix(neural_credit)\n",
    "confusion_matrix.fit(x_credit_training,y_credit_training)\n",
    "confusion_matrix.score(x_credit_test,y_credit_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       436\n",
      "           1       0.98      1.00      0.99        64\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       0.99      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_credit_test,prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('census.pkl', 'rb') as f:\n",
    "    x_census_training, y_census_training, x_census_test, y_census_test = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27676, 108), (27676,), (4885, 108), (4885,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_census_training.shape, y_census_training.shape, x_census_test.shape, y_census_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "108 input\n",
    "\n",
    "1 output (only 2 classes)\n",
    "\n",
    "108 + 1 = 109/2\n",
    "\n",
    "obs: usually in datasest with people information, 2 hidden layers are enough (both credit and census cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.39310281\n",
      "Iteration 2, loss = 0.32477347\n",
      "Iteration 3, loss = 0.31407656\n",
      "Iteration 4, loss = 0.30707966\n",
      "Iteration 5, loss = 0.30207012\n",
      "Iteration 6, loss = 0.29833718\n",
      "Iteration 7, loss = 0.29593788\n",
      "Iteration 8, loss = 0.29317703\n",
      "Iteration 9, loss = 0.29094209\n",
      "Iteration 10, loss = 0.28803881\n",
      "Iteration 11, loss = 0.28635245\n",
      "Iteration 12, loss = 0.28424196\n",
      "Iteration 13, loss = 0.28248519\n",
      "Iteration 14, loss = 0.27992921\n",
      "Iteration 15, loss = 0.27778261\n",
      "Iteration 16, loss = 0.27669393\n",
      "Iteration 17, loss = 0.27545591\n",
      "Iteration 18, loss = 0.27314065\n",
      "Iteration 19, loss = 0.27151905\n",
      "Iteration 20, loss = 0.26979026\n",
      "Iteration 21, loss = 0.26861364\n",
      "Iteration 22, loss = 0.26727333\n",
      "Iteration 23, loss = 0.26550794\n",
      "Iteration 24, loss = 0.26348409\n",
      "Iteration 25, loss = 0.26165448\n",
      "Iteration 26, loss = 0.26033023\n",
      "Iteration 27, loss = 0.25915380\n",
      "Iteration 28, loss = 0.25690846\n",
      "Iteration 29, loss = 0.25545710\n",
      "Iteration 30, loss = 0.25509688\n",
      "Iteration 31, loss = 0.25323232\n",
      "Iteration 32, loss = 0.25132762\n",
      "Iteration 33, loss = 0.24952798\n",
      "Iteration 34, loss = 0.24928709\n",
      "Iteration 35, loss = 0.24781890\n",
      "Iteration 36, loss = 0.24612035\n",
      "Iteration 37, loss = 0.24508882\n",
      "Iteration 38, loss = 0.24340871\n",
      "Iteration 39, loss = 0.24266530\n",
      "Iteration 40, loss = 0.24131045\n",
      "Iteration 41, loss = 0.24008275\n",
      "Iteration 42, loss = 0.23947289\n",
      "Iteration 43, loss = 0.23828004\n",
      "Iteration 44, loss = 0.23775246\n",
      "Iteration 45, loss = 0.23599478\n",
      "Iteration 46, loss = 0.23472135\n",
      "Iteration 47, loss = 0.23395598\n",
      "Iteration 48, loss = 0.23265624\n",
      "Iteration 49, loss = 0.23226798\n",
      "Iteration 50, loss = 0.23108897\n",
      "Iteration 51, loss = 0.22967325\n",
      "Iteration 52, loss = 0.22863336\n",
      "Iteration 53, loss = 0.22736778\n",
      "Iteration 54, loss = 0.22693076\n",
      "Iteration 55, loss = 0.22561906\n",
      "Iteration 56, loss = 0.22524352\n",
      "Iteration 57, loss = 0.22304931\n",
      "Iteration 58, loss = 0.22322573\n",
      "Iteration 59, loss = 0.22251993\n",
      "Iteration 60, loss = 0.22216197\n",
      "Iteration 61, loss = 0.22119884\n",
      "Iteration 62, loss = 0.21933041\n",
      "Iteration 63, loss = 0.21866255\n",
      "Iteration 64, loss = 0.21759550\n",
      "Iteration 65, loss = 0.21678364\n",
      "Iteration 66, loss = 0.21655420\n",
      "Iteration 67, loss = 0.21598513\n",
      "Iteration 68, loss = 0.21450529\n",
      "Iteration 69, loss = 0.21393480\n",
      "Iteration 70, loss = 0.21299667\n",
      "Iteration 71, loss = 0.21292033\n",
      "Iteration 72, loss = 0.21275524\n",
      "Iteration 73, loss = 0.21099667\n",
      "Iteration 74, loss = 0.20971204\n",
      "Iteration 75, loss = 0.21008229\n",
      "Iteration 76, loss = 0.20964638\n",
      "Iteration 77, loss = 0.20852138\n",
      "Iteration 78, loss = 0.20880832\n",
      "Iteration 79, loss = 0.20810745\n",
      "Iteration 80, loss = 0.20730527\n",
      "Iteration 81, loss = 0.20649603\n",
      "Iteration 82, loss = 0.20550649\n",
      "Iteration 83, loss = 0.20491329\n",
      "Iteration 84, loss = 0.20402761\n",
      "Iteration 85, loss = 0.20447489\n",
      "Iteration 86, loss = 0.20325086\n",
      "Iteration 87, loss = 0.20229760\n",
      "Iteration 88, loss = 0.20177832\n",
      "Iteration 89, loss = 0.20121419\n",
      "Iteration 90, loss = 0.20009963\n",
      "Iteration 91, loss = 0.20027896\n",
      "Iteration 92, loss = 0.19926979\n",
      "Iteration 93, loss = 0.19893732\n",
      "Iteration 94, loss = 0.19962825\n",
      "Iteration 95, loss = 0.19934796\n",
      "Iteration 96, loss = 0.19675494\n",
      "Iteration 97, loss = 0.19595847\n",
      "Iteration 98, loss = 0.19745682\n",
      "Iteration 99, loss = 0.19510623\n",
      "Iteration 100, loss = 0.19602702\n",
      "Iteration 101, loss = 0.19443613\n",
      "Iteration 102, loss = 0.19485076\n",
      "Iteration 103, loss = 0.19508087\n",
      "Iteration 104, loss = 0.19414465\n",
      "Iteration 105, loss = 0.19423088\n",
      "Iteration 106, loss = 0.19257575\n",
      "Iteration 107, loss = 0.19209476\n",
      "Iteration 108, loss = 0.19166071\n",
      "Iteration 109, loss = 0.19686621\n",
      "Iteration 110, loss = 0.19038155\n",
      "Iteration 111, loss = 0.18988441\n",
      "Iteration 112, loss = 0.18970415\n",
      "Iteration 113, loss = 0.18910540\n",
      "Iteration 114, loss = 0.18835559\n",
      "Iteration 115, loss = 0.18827965\n",
      "Iteration 116, loss = 0.18932922\n",
      "Iteration 117, loss = 0.18694525\n",
      "Iteration 118, loss = 0.18682188\n",
      "Iteration 119, loss = 0.18638277\n",
      "Iteration 120, loss = 0.18621416\n",
      "Iteration 121, loss = 0.18718303\n",
      "Iteration 122, loss = 0.18591305\n",
      "Iteration 123, loss = 0.18611172\n",
      "Iteration 124, loss = 0.18597174\n",
      "Iteration 125, loss = 0.18397379\n",
      "Iteration 126, loss = 0.18560204\n",
      "Iteration 127, loss = 0.18456654\n",
      "Iteration 128, loss = 0.18380666\n",
      "Iteration 129, loss = 0.18441988\n",
      "Iteration 130, loss = 0.18192110\n",
      "Iteration 131, loss = 0.18324058\n",
      "Iteration 132, loss = 0.18151297\n",
      "Iteration 133, loss = 0.18230131\n",
      "Iteration 134, loss = 0.18148422\n",
      "Iteration 135, loss = 0.18054245\n",
      "Iteration 136, loss = 0.18006569\n",
      "Iteration 137, loss = 0.17976280\n",
      "Iteration 138, loss = 0.17987698\n",
      "Iteration 139, loss = 0.17884848\n",
      "Iteration 140, loss = 0.17875916\n",
      "Iteration 141, loss = 0.17978078\n",
      "Iteration 142, loss = 0.17822872\n",
      "Iteration 143, loss = 0.17865194\n",
      "Iteration 144, loss = 0.17754437\n",
      "Iteration 145, loss = 0.17754836\n",
      "Iteration 146, loss = 0.17819257\n",
      "Iteration 147, loss = 0.17586683\n",
      "Iteration 148, loss = 0.17715362\n",
      "Iteration 149, loss = 0.17549828\n",
      "Iteration 150, loss = 0.17672788\n",
      "Iteration 151, loss = 0.17675767\n",
      "Iteration 152, loss = 0.17634542\n",
      "Iteration 153, loss = 0.17447209\n",
      "Iteration 154, loss = 0.17461766\n",
      "Iteration 155, loss = 0.17408131\n",
      "Iteration 156, loss = 0.17400669\n",
      "Iteration 157, loss = 0.17371804\n",
      "Iteration 158, loss = 0.17403751\n",
      "Iteration 159, loss = 0.17311276\n",
      "Iteration 160, loss = 0.17261618\n",
      "Iteration 161, loss = 0.17286318\n",
      "Iteration 162, loss = 0.17207193\n",
      "Iteration 163, loss = 0.17229901\n",
      "Iteration 164, loss = 0.17337881\n",
      "Iteration 165, loss = 0.17312958\n",
      "Iteration 166, loss = 0.17013441\n",
      "Iteration 167, loss = 0.17000791\n",
      "Iteration 168, loss = 0.17018996\n",
      "Iteration 169, loss = 0.17038422\n",
      "Iteration 170, loss = 0.17163601\n",
      "Iteration 171, loss = 0.16924696\n",
      "Iteration 172, loss = 0.16974006\n",
      "Iteration 173, loss = 0.16886150\n",
      "Iteration 174, loss = 0.16887336\n",
      "Iteration 175, loss = 0.17028259\n",
      "Iteration 176, loss = 0.16751239\n",
      "Iteration 177, loss = 0.16784102\n",
      "Iteration 178, loss = 0.16749432\n",
      "Iteration 179, loss = 0.16725793\n",
      "Iteration 180, loss = 0.16602105\n",
      "Iteration 181, loss = 0.16729337\n",
      "Iteration 182, loss = 0.16711786\n",
      "Iteration 183, loss = 0.16561593\n",
      "Iteration 184, loss = 0.16621844\n",
      "Iteration 185, loss = 0.16585729\n",
      "Iteration 186, loss = 0.16592944\n",
      "Iteration 187, loss = 0.16750043\n",
      "Iteration 188, loss = 0.16413378\n",
      "Iteration 189, loss = 0.16448028\n",
      "Iteration 190, loss = 0.16470068\n",
      "Iteration 191, loss = 0.16476435\n",
      "Iteration 192, loss = 0.16443728\n",
      "Iteration 193, loss = 0.16426932\n",
      "Iteration 194, loss = 0.16449896\n",
      "Iteration 195, loss = 0.16386914\n",
      "Iteration 196, loss = 0.16474628\n",
      "Iteration 197, loss = 0.16596165\n",
      "Iteration 198, loss = 0.16248966\n",
      "Iteration 199, loss = 0.16361057\n",
      "Iteration 200, loss = 0.16354970\n",
      "Iteration 201, loss = 0.16368014\n",
      "Iteration 202, loss = 0.16253105\n",
      "Iteration 203, loss = 0.16073970\n",
      "Iteration 204, loss = 0.16106804\n",
      "Iteration 205, loss = 0.16097434\n",
      "Iteration 206, loss = 0.16111045\n",
      "Iteration 207, loss = 0.16005291\n",
      "Iteration 208, loss = 0.15921532\n",
      "Iteration 209, loss = 0.15917961\n",
      "Iteration 210, loss = 0.15858175\n",
      "Iteration 211, loss = 0.15984495\n",
      "Iteration 212, loss = 0.16091602\n",
      "Iteration 213, loss = 0.15989861\n",
      "Iteration 214, loss = 0.15842361\n",
      "Iteration 215, loss = 0.15833483\n",
      "Iteration 216, loss = 0.16054505\n",
      "Iteration 217, loss = 0.15949856\n",
      "Iteration 218, loss = 0.15868183\n",
      "Iteration 219, loss = 0.15891371\n",
      "Iteration 220, loss = 0.15921761\n",
      "Iteration 221, loss = 0.15825872\n",
      "Iteration 222, loss = 0.15697735\n",
      "Iteration 223, loss = 0.15699847\n",
      "Iteration 224, loss = 0.15867760\n",
      "Iteration 225, loss = 0.15691342\n",
      "Iteration 226, loss = 0.15728325\n",
      "Iteration 227, loss = 0.15613241\n",
      "Iteration 228, loss = 0.15711451\n",
      "Iteration 229, loss = 0.15672547\n",
      "Iteration 230, loss = 0.15643621\n",
      "Iteration 231, loss = 0.15606690\n",
      "Iteration 232, loss = 0.15535134\n",
      "Iteration 233, loss = 0.15651216\n",
      "Iteration 234, loss = 0.15574063\n",
      "Iteration 235, loss = 0.15658116\n",
      "Iteration 236, loss = 0.15502774\n",
      "Iteration 237, loss = 0.15458716\n",
      "Iteration 238, loss = 0.15460730\n",
      "Iteration 239, loss = 0.15452610\n",
      "Iteration 240, loss = 0.15555501\n",
      "Iteration 241, loss = 0.15493438\n",
      "Iteration 242, loss = 0.15337516\n",
      "Iteration 243, loss = 0.15444195\n",
      "Iteration 244, loss = 0.15580371\n",
      "Iteration 245, loss = 0.15408556\n",
      "Iteration 246, loss = 0.15407228\n",
      "Iteration 247, loss = 0.15302349\n",
      "Iteration 248, loss = 0.15319917\n",
      "Iteration 249, loss = 0.15300961\n",
      "Iteration 250, loss = 0.15270513\n",
      "Iteration 251, loss = 0.15313896\n",
      "Iteration 252, loss = 0.15265579\n",
      "Iteration 253, loss = 0.15233591\n",
      "Iteration 254, loss = 0.15191097\n",
      "Iteration 255, loss = 0.15226173\n",
      "Iteration 256, loss = 0.15173538\n",
      "Iteration 257, loss = 0.15153966\n",
      "Iteration 258, loss = 0.15184391\n",
      "Iteration 259, loss = 0.15214966\n",
      "Iteration 260, loss = 0.15125749\n",
      "Iteration 261, loss = 0.15052401\n",
      "Iteration 262, loss = 0.15028103\n",
      "Iteration 263, loss = 0.15129321\n",
      "Iteration 264, loss = 0.15046787\n",
      "Iteration 265, loss = 0.15012536\n",
      "Iteration 266, loss = 0.15130183\n",
      "Iteration 267, loss = 0.14995493\n",
      "Iteration 268, loss = 0.15025182\n",
      "Iteration 269, loss = 0.14975674\n",
      "Iteration 270, loss = 0.15078399\n",
      "Iteration 271, loss = 0.14941119\n",
      "Iteration 272, loss = 0.14916888\n",
      "Iteration 273, loss = 0.15012229\n",
      "Iteration 274, loss = 0.14866228\n",
      "Iteration 275, loss = 0.14996298\n",
      "Iteration 276, loss = 0.15193562\n",
      "Iteration 277, loss = 0.14844681\n",
      "Iteration 278, loss = 0.14825807\n",
      "Iteration 279, loss = 0.14763536\n",
      "Iteration 280, loss = 0.14887241\n",
      "Iteration 281, loss = 0.14678458\n",
      "Iteration 282, loss = 0.14848601\n",
      "Iteration 283, loss = 0.14603826\n",
      "Iteration 284, loss = 0.14555707\n",
      "Iteration 285, loss = 0.14736888\n",
      "Iteration 286, loss = 0.14802119\n",
      "Iteration 287, loss = 0.14762370\n",
      "Iteration 288, loss = 0.14719210\n",
      "Iteration 289, loss = 0.14721367\n",
      "Iteration 290, loss = 0.14820713\n",
      "Iteration 291, loss = 0.14631517\n",
      "Iteration 292, loss = 0.14843421\n",
      "Iteration 293, loss = 0.14664519\n",
      "Iteration 294, loss = 0.14661474\n",
      "Iteration 295, loss = 0.14679175\n",
      "Training loss did not improve more than tol=0.000010 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-10 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-10 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-10 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-10 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-10 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-10 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-10 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-10 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-10 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-10 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-10 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-10 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-10 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-10 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-10\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" checked><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;MLPClassifier<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.neural_network.MLPClassifier.html\">?<span>Documentation for MLPClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(55, 55), max_iter=1000, tol=1e-05,\n",
       "              verbose=True)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_census = MLPClassifier(max_iter = 1000, \n",
    "                              verbose = True, # True show the loss for each iteration\n",
    "                              tol = 0.0000100,\n",
    "                              solver = 'adam',\n",
    "                              activation = 'relu',\n",
    "                              hidden_layer_sizes = (55,55)# 108 neurons input -> 55 hidden -> 55 hidden -> 1 neuron output\n",
    "                                ) \n",
    "neural_census.fit(x_census_training,y_census_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8114636642784033"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = neural_census.predict(x_census_test)\n",
    "accuracy_score(y_census_test,prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8114636642784033"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x550 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "confusion_matrix = ConfusionMatrix(neural_census)\n",
    "confusion_matrix.fit(x_census_training,y_census_training)\n",
    "confusion_matrix.score(x_census_test,y_census_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       <=50K       0.88      0.87      0.87      3693\n",
      "        >50K       0.61      0.63      0.62      1192\n",
      "\n",
      "    accuracy                           0.81      4885\n",
      "   macro avg       0.74      0.75      0.75      4885\n",
      "weighted avg       0.81      0.81      0.81      4885\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_census_test,prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
